{
  "hash": "5d35aa8edea8240d95bb52ded0fc8ce1",
  "result": {
    "markdown": "---\ntitle: \"Poisson Regression Examples\"\nauthor: \"Charlotte Hunter\"\ndate: today\ncallout-appearance: minimal # this hides the blue \"i\" icon on .callout-notes\ncode-fold: true       # Makes all code chunks collapsible\ncode-tools: true\nformat: html\n---\n\n\n## Blueprinty Case Study\n\n### Introduction\n\nBlueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. \n\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.\n\n\n### Data\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# histogram of patents\nggplot(blueprinty, aes(x = patents, fill = factor(iscustomer))) +\n  geom_histogram(binwidth = 1, position = \"dodge\", alpha = 0.7) +\n  labs(\n    title = \"Histogram of Patents by Customer Status\",\n    x = \"Number of Patents\",\n    y = \"Count\",\n    fill = \"Customer Status\"\n  ) +\n  scale_fill_manual(values = c(\"0\" = \"gray\", \"1\" = \"steelblue\"),\n                    labels = c(\"0\" = \"Non-Customer\", \"1\" = \"Customer\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# average patents table\nblueprinty %>%\n  group_by(iscustomer) %>%\n  summarize(mean_patents = mean(patents, na.rm = TRUE)) %>%\n  mutate(\n    iscustomer = ifelse(iscustomer == 1, \"Customer\", \"Non-Customer\")\n  ) %>%\n  kable(digits = 2, col.names = c(\"Customer Status\", \"Average Number of Patents\"),\n        caption = \"Average Patents by Customer Status\")\n```\n\n::: {.cell-output-display}\nTable: Average Patents by Customer Status\n\n|Customer Status | Average Number of Patents|\n|:---------------|-------------------------:|\n|Non-Customer    |                      3.47|\n|Customer        |                      4.13|\n:::\n:::\n\n\nCustomers tend to have a higher number of patents than non-customers. The average for customers is 4.13 patents, compared to 3.47 for non-customers. The histogram shows that customers are more likely to have higher patent counts, with their distribution shifted to the right. Non-customers are more concentrated in the 0–3 patent range, while customers are more represented in the 4–8 range. This suggests a positive relationship between patent ownership and the likelihood of being a customer.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# boxplot age distribution\nggplot(blueprinty, aes(x = factor(iscustomer), y = age, fill = factor(iscustomer))) +\n  geom_boxplot() +\n  labs(\n    title = \"Age Distribution by Customer Status\",\n    x = \"Customer Status\",\n    y = \"Age\",\n    fill = \"Customer\"\n  ) +\n  scale_fill_manual(values = c(\"0\" = \"gray\", \"1\" = \"steelblue\"),\n                    labels = c(\"0\" = \"Non-Customer\", \"1\" = \"Customer\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# mean age table\nblueprinty %>%\n  group_by(iscustomer) %>%\n  summarize(mean_age = mean(age, na.rm = TRUE)) %>%\n  mutate(\n    iscustomer = ifelse(iscustomer == 1, \"Customer\", \"Non-Customer\")\n  ) %>%\n  kable(digits = 2, col.names = c(\"Customer Status\", \"Average Age\"),\n        caption = \"Average Age by Customer Status\")\n```\n\n::: {.cell-output-display}\nTable: Average Age by Customer Status\n\n|Customer Status | Average Age|\n|:---------------|-----------:|\n|Non-Customer    |        26.1|\n|Customer        |        26.9|\n:::\n:::\n\n\nCustomers are slightly older than non-customers on average (26.9 vs. 26.1 years). The boxplot shows similar distributions, but customers have a slightly higher median and a longer upper tail. This suggests that customer status is somewhat associated with age, though the difference is modest.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# proportions for each region by customer status\nregion_customer_prop <- blueprinty %>%\n  group_by(region, iscustomer) %>%\n  summarise(count = n(), .groups = \"drop\") %>%\n  group_by(region) %>%\n  mutate(prop = count / sum(count)) %>%\n  ungroup() %>%\n  mutate(iscustomer = factor(iscustomer, labels = c(\"Non-Customer\", \"Customer\")))\n\n# bar plot \nggplot(region_customer_prop, aes(x = region, y = prop, fill = iscustomer)) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.7)) +\n  labs(\n    title = \"Proportion of Customers and Non-Customers by Region\",\n    x = \"Region\",\n    y = \"Proportion\",\n    fill = \"Customer Status\"\n  ) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nRegional differences are more pronounced. In the Northeast, over half of the firms are customers — a much higher proportion than any other region. In contrast, regions like the Midwest, Northwest, South, and Southwest have much lower customer proportions (generally below 20%). This suggests strong regional variation in customer conversion, with the Northeast standing out as Blueprinty’s most successful market.\n\n### Estimation of Simple Poisson Model\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\nAssume we have \\( n \\) independent observations \\( Y_1, Y_2, \\dots, Y_n \\), each distributed as:\n\n$$\nY_i \\sim \\text{Poisson}(\\lambda)\n$$\n\nThe probability mass function is:\n\n$$\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n$$\n\nSince the observations are independent, the **likelihood function** is the product of their individual densities:\n\n$$\n\\mathcal{L}(\\lambda \\mid Y_1, \\dots, Y_n) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n$$\n\nThis simplifies to:\n\n$$\n\\mathcal{L}(\\lambda) = \\frac{e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n Y_i}}{\\prod_{i=1}^n Y_i!}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# log-likelihood function for Poisson model\npoisson_loglikelihood <- function(lambda, Y) {\n  if (lambda <= 0) return(-Inf)  # Invalid for log\n  n <- length(Y)\n  ll <- -n * lambda + sum(Y) * log(lambda) - sum(lgamma(Y + 1))\n  return(ll)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# use the observed number \ny_data <- blueprinty$patents\n\n# define a sequence of lambda values\nlambda_vals <- seq(0.1, 10, length.out = 200)\n\n# log-likelihoods\nlog_likelihoods <- sapply(lambda_vals, function(l) poisson_loglikelihood(l, y_data))\n\n# plot\nplot(lambda_vals, log_likelihoods, type = \"l\",\n     main = \"Log-Likelihood of Poisson Model\",\n     xlab = expression(lambda),\n     ylab = \"Log-Likelihood\",\n     col = \"steelblue\", lwd = 2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nWe start with the log-likelihood function for \\( n \\) independent observations from a Poisson distribution:\n\n$$\n\\log \\mathcal{L}(\\lambda) = -n\\lambda + \\left(\\sum_{i=1}^n Y_i\\right) \\log \\lambda - \\sum_{i=1}^n \\log(Y_i!)\n$$\n\nTaking the derivative with respect to \\( \\lambda \\):\n\n$$\n\\frac{d}{d\\lambda} \\log \\mathcal{L}(\\lambda) = -n + \\frac{\\sum_{i=1}^n Y_i}{\\lambda}\n$$\n\nSetting the derivative equal to zero:\n\n$$\n-n + \\frac{\\sum_{i=1}^n Y_i}{\\lambda} = 0\n\\quad \\Rightarrow \\quad\n\\hat{\\lambda} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n$$\n\nThis result makes intuitive sense, as the mean of a Poisson distribution is \\(\\lambda\\), so the MLE is simply the sample mean.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# negative log-likelihood wrapper for use with optim()\nneg_log_likelihood_poisson <- function(lambda, Y) {\n  if (lambda <= 0) return(Inf)\n  -poisson_loglikelihood(lambda, Y)\n}\n\n# use optim to find MLE\noptim_result <- optim(\n  par = 1,  # initial guess\n  fn = neg_log_likelihood_poisson,\n  Y = blueprinty$patents,\n  method = \"Brent\",\n  lower = 0.001,\n  upper = 10\n)\n\n# extract MLE estimate\nlambda_mle_optim <- optim_result$par\ncat(\"MLE estimate of lambda:\", round(lambda_mle_optim, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMLE estimate of lambda: 3.685 \n```\n:::\n:::\n\n\n### Estimation of Poisson Regression Model\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \\text{Poisson}(\\lambda_i)$ where $\\lambda_i = \\exp(X_i'\\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# log-likelihood for Poisson regression\npoisson_regression_loglik <- function(beta, Y, X) {\n  eta <- X %*% beta                    \n  lambda <- exp(eta)                  \n  ll <- sum(Y * log(lambda) - lambda - lgamma(Y + 1))\n  return(ll)\n}\n```\n:::\n\n\n_todo: Use your function along with R's optim() or Python's sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1's to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors._\n\n\n::: {.cell}\n\n```{.r .cell-code}\nblueprinty$age2 <- blueprinty$age^2\n\n# region dummy variables, dropping one region\nregion_dummies <- model.matrix(~ region, data = blueprinty)[, -1]  # drops intercept & first level\n\n# matrix\nX <- cbind(Intercept = 1,\n           age = blueprinty$age,\n           age2 = blueprinty$age2,\n           region_dummies,\n           iscustomer = blueprinty$iscustomer)\n\nY <- blueprinty$patents\n\n# define log-likelihood function\nneg_loglik <- function(beta, Y, X) {\n  eta <- X %*% beta\n  lambda <- exp(eta)\n  -sum(Y * log(lambda) - lambda - lgamma(Y + 1))  # negative\n}\n\n# optimize using optim()\ninit_beta <- rep(0, ncol(X))  \nfit <- optim(\n  par = init_beta,\n  fn = neg_loglik,\n  Y = Y,\n  X = X,\n  hessian = TRUE,\n  method = \"BFGS\"\n)\n\n# extract coefficients & standard errors\nbeta_hat <- fit$par\nhessian <- fit$hessian\nse_beta <- sqrt(diag(solve(hessian)))\n\n# fix possible duplicate column names\ncolnames_X <- make.names(colnames(X), unique = TRUE)\n\n# create & display coefficient table\ncoef_table <- data.frame(\n  Term = colnames_X,\n  Coefficient = beta_hat,\n  Std_Error = se_beta\n)\n\nknitr::kable(coef_table, digits = 3, caption = \"Poisson Regression Coefficients and Standard Errors\")\n```\n\n::: {.cell-output-display}\nTable: Poisson Regression Coefficients and Standard Errors\n\n|Term            | Coefficient| Std_Error|\n|:---------------|-----------:|---------:|\n|Intercept       |      -0.126|     0.112|\n|age             |       0.116|     0.006|\n|age2            |      -0.002|     0.000|\n|regionNortheast |      -0.025|     0.043|\n|regionNorthwest |      -0.035|     0.053|\n|regionSouth     |      -0.005|     0.052|\n|regionSouthwest |      -0.038|     0.047|\n|iscustomer      |       0.061|     0.032|\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit Poisson regression using glm()\nglm_fit <- glm(\n  patents ~ age + I(age^2) + region + iscustomer,\n  data = blueprinty,\n  family = poisson(link = \"log\")\n)\n\nsummary(glm_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + iscustomer, \n    family = poisson(link = \"log\"), data = blueprinty)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(>|z|)    \n(Intercept)     -0.508920   0.183179  -2.778  0.00546 ** \nage              0.148619   0.013869  10.716  < 2e-16 ***\nI(age^2)        -0.002971   0.000258 -11.513  < 2e-16 ***\nregionNortheast  0.029170   0.043625   0.669  0.50372    \nregionNorthwest -0.017574   0.053781  -0.327  0.74383    \nregionSouth      0.056561   0.052662   1.074  0.28281    \nregionSouthwest  0.050576   0.047198   1.072  0.28391    \niscustomer       0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n```\n:::\n:::\n\n\nThe Poisson regression results show that age and customer status are strong predictors of patent counts. Age has a positive effect, but the negative coefficient on age² indicates diminishing returns, suggesting patent activity peaks at a certain age. Being a customer increases expected patent counts by approximately 23%, holding other factors constant. Regional effects are not statistically significant, indicating that after adjusting for age and customer status, geography does not explain substantial variation in patent activity.\n\n::: {.callout-note title=\"Poisson Regression Summary\"}\n- Age: Significant positive effect with diminishing returns (non-linear curve due to age²)\n- Customer status: Customers have ~23% more patents than non-customers (exp(0.208))\n- Region: No region has a statistically significant effect on patent counts\n- Model insight: Patent activity is best explained by age (with a peak) and customer conversion\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX_base <- cbind(\n  Intercept = 1,\n  age = blueprinty$age,\n  age2 = blueprinty$age^2,\n  model.matrix(~ region, data = blueprinty)[, -1],  # region dummies (drop intercept)\n  iscustomer = blueprinty$iscustomer\n)\n\n# create datasets\nX_0 <- X_base\nX_0[, \"iscustomer\"] <- 0\n\nX_1 <- X_base\nX_1[, \"iscustomer\"] <- 1\n\n# use beta_hat (from optim) \neta_0 <- X_0 %*% beta_hat\neta_1 <- X_1 %*% beta_hat\n\ny_pred_0 <- exp(eta_0)\ny_pred_1 <- exp(eta_1)\n\n# average difference \navg_diff <- mean(y_pred_1 - y_pred_0)\ncat(\"Estimated average increase:\", round(avg_diff, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEstimated average increase: 0.218 \n```\n:::\n:::\n\n\n## AirBnB Case Study\n\n### Introduction\n\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:\n\n:::: {.callout-note collapse=\"true\"}\n### Variable Definitions\n\n    - `id` = unique ID number for each unit\n    - `last_scraped` = date when information scraped\n    - `host_since` = date when host first listed the unit on Airbnb\n    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n    - `room_type` = Entire home/apt., Private room, or Shared room\n    - `bathrooms` = number of bathrooms\n    - `bedrooms` = number of bedrooms\n    - `price` = price per night (dollars)\n    - `number_of_reviews` = number of reviews for the unit on Airbnb\n    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n    - `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n    - `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n    - `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n::::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# glm() to fit Poisson model\nairbnb_model <- glm(\n  number_of_reviews ~ bathrooms + bedrooms + price +\n    review_scores_cleanliness + review_scores_location + review_scores_value +\n    instant_bookable + room_type,\n  data = airbnb_clean,\n  family = poisson(link = \"log\")\n)\n\nsummary(airbnb_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = number_of_reviews ~ bathrooms + bedrooms + price + \n    review_scores_cleanliness + review_scores_location + review_scores_value + \n    instant_bookable + room_type, family = poisson(link = \"log\"), \n    data = airbnb_clean)\n\nCoefficients: (1 not defined because of singularities)\n                            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                3.714e+00  1.587e-02 233.976  < 2e-16 ***\nbathrooms                 -1.164e-01  3.786e-03 -30.751  < 2e-16 ***\nbedrooms                   7.607e-02  2.001e-03  38.016  < 2e-16 ***\nprice                     -3.275e-05  8.521e-06  -3.843 0.000121 ***\nreview_scores_cleanliness  1.140e-01  1.486e-03  76.750  < 2e-16 ***\nreview_scores_location    -8.065e-02  1.599e-03 -50.445  < 2e-16 ***\nreview_scores_value       -9.749e-02  1.789e-03 -54.484  < 2e-16 ***\ninstant_bookable                  NA         NA      NA       NA    \nroom_typePrivate room      7.405e-03  2.734e-03   2.709 0.006747 ** \nroom_typeShared room      -2.262e-01  8.616e-03 -26.249  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 961626  on 30159  degrees of freedom\nResidual deviance: 949198  on 30151  degrees of freedom\nAIC: 1070683\n\nNumber of Fisher Scoring iterations: 6\n```\n:::\n:::\n\n\nThis Poisson regression model examines how listing characteristics affect the number of reviews, used here as a proxy for bookings. The results show that bedroom count and review cleanliness scores have strong positive effects on review count, indicating that larger and cleaner listings attract more bookings.\n\nHigher prices are associated with fewer reviews, suggesting price sensitivity among renters. Interestingly, while location and value scores are typically seen as positive quality signals, they have significant negative coefficients — which may reflect reverse causality, data quality issues, or confounding factors (e.g., higher-scoring locations already having high competition or price).\n\nRoom type also plays a role: private rooms receive more reviews than entire homes (the baseline), while shared rooms receive fewer. The instant_bookable variable was excluded due to collinearity or lack of variation.\n\nOverall, the model confirms that listing attributes, epecially quality signals and room configuration, meaningfully influence booking behavior on the platform.\n\n::: {.callout-note title=\"Key Findings\"}\n- Bathrooms: More bathrooms → fewer reviews (surprising, possibly due to correlation with higher-end/less frequently booked properties)\n- Bedrooms: More bedrooms → more reviews (larger listings are more popular)\n- Price: Higher price → fewer reviews (suggests price sensitivity among renters)\n- Review scores:\n  - Cleanliness: Higher score → more reviews (clear signal of quality)\n  - Location & Value: Higher scores → fewer reviews (unexpected; may reflect underlying data quirks)\n- Room type:\n  - Private room → more reviews than entire homes\n  - Shared room → fewer reviews than entire homes\n- Instant bookable: Dropped due to collinearity or lack of variation; revisit in preprocessing if needed\n:::\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}