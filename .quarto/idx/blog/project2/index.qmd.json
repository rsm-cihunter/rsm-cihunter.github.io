{"title":"Multinomial Logit Model","markdown":{"yaml":{"title":"Multinomial Logit Model","author":"Charlotte Hunter","date":"today","callout-appearance":"minimal","code-fold":true,"code-tools":true,"format":"html"},"headingText":"1. Likelihood for the Multi-nomial Logit (MNL) Model","containsRefs":false,"markdown":"\n\nThis assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm. \n\n\nSuppose we have $i=1,\\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \\in \\{1, \\ldots, J\\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 3 products, then either $y=3$ or $y=(0,0,1)$ depending on how we want to represent it. Suppose also that we have a vector of data on each product $x_j$ (eg, brand, price, etc.). \n\nWe model the consumer's decision as the selection of the product that provides the most utility, and we'll specify the utility function as a linear function of the product characteristics:\n\n$$ U_{ij} = x_j'\\beta + \\epsilon_{ij} $$\n\nwhere $\\epsilon_{ij}$ is an i.i.d. extreme value error term. \n\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer $i$ chooses product $j$:\n\n$$ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} $$\n\nFor example, if there are 3 products, the probability that consumer $i$ chooses product 3 is:\n\n$$ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} $$\n\nA clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\\delta_{ij}$) that indicates the chosen product:\n\n$$ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}$$\n\nNotice that if the consumer selected product $j=3$, then $\\delta_{i3}=1$ while $\\delta_{i1}=\\delta_{i2}=0$ and the likelihood is:\n\n$$ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} $$\n\nThe joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:\n\n$$ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} $$\n\nAnd the joint log-likelihood function is:\n\n$$ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) $$\n\n\n\n## 2. Simulate Conjoint Data\n\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a \"no choice\" option; each simulated respondent must select one of the 3 alternatives. \n\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from \\$4 to \\$32 in increments of \\$4.\n\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer $i$ for hypothethical streaming service $j$ is \n\n$$\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n$$\n\nwhere the variables are binary indicators and $\\varepsilon$ is Type 1 Extreme Value (ie, Gumble) distributed.\n\nThe following code provides the simulation of the conjoint data based on the utility specification described above. Each respondent faces a series of choice tasks, and their selections are determined by both the systematic utility from product attributes and a random error term drawn from a Gumbel distribution. This setup mimics how real respondents might make decisions in a discrete choice experiment.\n\n:::: {.callout-note collapse=\"true\"}\n```{r}\n# set seed for reproducibility\nset.seed(123)\n\n# define attributes\nbrand <- c(\"N\", \"P\", \"H\") # Netflix, Prime, Hulu\nad <- c(\"Yes\", \"No\")\nprice <- seq(8, 32, by=4)\n\n# generate all possible profiles\nprofiles <- expand.grid(\n    brand = brand,\n    ad = ad,\n    price = price\n)\nm <- nrow(profiles)\n\n# assign part-worth utilities (true parameters)\nb_util <- c(N = 1.0, P = 0.5, H = 0)\na_util <- c(Yes = -0.8, No = 0.0)\np_util <- function(p) -0.1 * p\n\n# number of respondents, choice tasks, and alternatives per task\nn_peeps <- 100\nn_tasks <- 10\nn_alts <- 3\n\n# function to simulate one respondent’s data\nsim_one <- function(id) {\n  \n    datlist <- list()\n    \n    # loop over choice tasks\n    for (t in 1:n_tasks) {\n        \n        # randomly sample 3 alts (better practice would be to use a design)\n        dat <- cbind(resp=id, task=t, profiles[sample(m, size=n_alts), ])\n        \n        # compute deterministic portion of utility\n        dat$v <- b_util[dat$brand] + a_util[dat$ad] + p_util(dat$price) |> round(10)\n        \n        # add Gumbel noise (Type I extreme value)\n        dat$e <- -log(-log(runif(n_alts)))\n        dat$u <- dat$v + dat$e\n        \n        # identify chosen alternative\n        dat$choice <- as.integer(dat$u == max(dat$u))\n        \n        # store task\n        datlist[[t]] <- dat\n    }\n    \n    # combine all tasks for one respondent\n    do.call(rbind, datlist)\n}\n\n# simulate data for all respondents\nconjoint_data <- do.call(rbind, lapply(1:n_peeps, sim_one))\n\n# remove values unobservable to the researcher\nconjoint_data <- conjoint_data[ , c(\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\")]\n\n# clean up\nrm(list=setdiff(ls(), \"conjoint_data\"))\n```\n::::\n\n## 3. Preparing the Data for Estimation\n\nThe \"hard part\" of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer $i$, covariate $k$, and product $j$) instead of the typical 2 dimensions for cross-sectional regression models (consumer $i$ and covariate $k$). The fact that each task for each respondent has the same number of alternatives (3) helps.  In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\nIn order to estimate the MNL model, we need to reshape the data so that it includes one row per alternative, per choice task, per respondent. We also convert categorical variables (brand and ad format) into binary indicators using dummy coding, with Hulu and ad-free as the reference categories. This format allows us to calculate utility values and model choice behavior based on attribute-level part-worths.\n\n```{r, warning=FALSE, message=FALSE}\n# required libraries\nlibrary(dplyr)\nlibrary(tidyr)\n\n# work with simulated conjoint_data\ndf <- conjoint_data\n\n# convert to factors with reference levels\ndf$brand <- relevel(factor(df$brand), ref = \"H\")  # Hulu is reference\ndf$ad <- relevel(factor(df$ad), ref = \"No\")       # No ads is reference\n\n# create dummy variables, no intercept to avoid collinearity\nX <- model.matrix(~ brand + ad - 1, data = df)\nX <- as.data.frame(X)\n\n# rename to match previous convention\ncolnames(X) <- gsub(\"brandN\", \"brand_N\", colnames(X))\ncolnames(X) <- gsub(\"brandP\", \"brand_P\", colnames(X))\ncolnames(X) <- gsub(\"adYes\", \"ad_Yes\", colnames(X))  # after setting \"No\" as reference, this should now appear\n\n# combine and reorder\ndf <- bind_cols(df %>% select(resp, task, price, choice), X)\ndf <- df %>% select(resp, task, brand_N, brand_P, ad_Yes, price, choice)\n```\n\n## 4. Estimation via Maximum Likelihood\n\nTo estimate the parameters of the MNL model, we define a log-likelihood function that calculates the probability of each observed choice based on a given set of \nβ\nβ coefficients. The function computes the utility of each alternative, transforms it into a probability using the softmax formula, and sums the log-probabilities of the chosen alternatives. This function is then used in optimization to find the best-fitting parameters.\n\n```{r, warning=FALSE, message=FALSE}\n# log-likelihood function for MNL\nlog_likelihood <- function(beta, data) {\n  # unpack beta coefficients\n  b_brand_N <- beta[1]\n  b_brand_P <- beta[2]\n  b_ad_Yes  <- beta[3]\n  b_price   <- beta[4]\n  \n  # compute linear utility for all alternatives\n  data$v <- b_brand_N * data$brand_N +\n            b_brand_P * data$brand_P +\n            b_ad_Yes  * data$ad_Yes +\n            b_price   * data$price\n  \n  # normalize by task (3 alternatives per task)\n  data <- data %>% \n    group_by(resp, task) %>%\n    mutate(\n      exp_v = exp(v),\n      denom = sum(exp_v),\n      p = exp_v / denom,\n      logp = log(p)\n    ) %>%\n    ungroup()\n  \n  # compute total log-likelihood over chosen alternatives\n  ll <- sum(data$choice * data$logp)\n  return(-ll)  # return negative log-likelihood for minimization\n}\n```\n\nWith the log-likelihood function defined, we now use the optim() function in R to find the maximum likelihood estimates of the four model parameters. The optimization uses the BFGS method and returns both the coefficient estimates and the Hessian matrix. We use the inverse of the Hessian to calculate standard errors and construct 95% confidence intervals for each parameter.\n\n```{r, warning=FALSE, message=FALSE}\n# initial guess for the parameters\nstart_vals <- c(0, 0, 0, 0)  # brand_N, brand_P, ad_Yes, price\n\n# run optimization using BFGS (returns Hessian)\nmle_result <- optim(par = start_vals,\n                    fn = log_likelihood,\n                    data = df,\n                    method = \"BFGS\",\n                    hessian = TRUE)\n\n# extract estimated coefficients\nbeta_hat <- mle_result$par\n\n# compute standard errors from inverse Hessian\nvar_cov_matrix <- solve(mle_result$hessian)\nstd_errors <- sqrt(diag(var_cov_matrix))\n\n# 95% confidence intervals\nz_value <- qnorm(0.975)  # for 95% CI\nlower_ci <- beta_hat - z_value * std_errors\nupper_ci <- beta_hat + z_value * std_errors\n\n# build  summary table\nmle_summary <- data.frame(\n  Parameter = c(\"brand_N\", \"brand_P\", \"ad_Yes\", \"price\"),\n  Estimate = beta_hat,\n  StdError = std_errors,\n  CI_Lower = lower_ci,\n  CI_Upper = upper_ci\n)\n\nmle_summary <- mle_summary %>%\n  mutate(across(Estimate:CI_Upper))\n\nknitr::kable(mle_summary, caption = \"Maximum Likelihood Estimates with 95% Confidence Intervals\")\n```\n\n## 5. Estimation via Bayesian Methods\n\nWe now estimate the MNL model using a Bayesian approach via the Metropolis-Hastings MCMC algorithm. We define weakly informative priors for the coefficients and generate 11,000 samples from the posterior distribution, discarding the first 1,000 as burn-in. The log-likelihood function from the MLE step is reused, and posterior draws are accepted or rejected based on the Metropolis-Hastings criterion. This approach gives us a full distribution for each parameter rather than just point estimates.\n\n```{r, warning=FALSE, message=FALSE}\n# log prior\nlog_prior <- function(beta) {\n  # brand_N, brand_P, ad_Yes ~ N(0, 5); price ~ N(0, 1)\n  lp <- dnorm(beta[1], 0, sqrt(5), log = TRUE) +\n        dnorm(beta[2], 0, sqrt(5), log = TRUE) +\n        dnorm(beta[3], 0, sqrt(5), log = TRUE) +\n        dnorm(beta[4], 0, 1, log = TRUE)\n  return(lp)\n}\n\n# log posterior = log-likelihood + log-prior\nlog_posterior <- function(beta, data) {\n  -log_likelihood(beta, data) + log_prior(beta)\n}\n\n# mcmc settings\nn_iter <- 11000\nburn_in <- 1000\nkeep <- n_iter - burn_in\n\n# storage\nbeta_samples <- matrix(NA, nrow = n_iter, ncol = 4)\ncolnames(beta_samples) <- c(\"brand_N\", \"brand_P\", \"ad_Yes\", \"price\")\n\n# initialize\nbeta_current <- c(0, 0, 0, 0)\nlog_post_current <- log_posterior(beta_current, df)\n\n# proposal sd (step size)\nproposal_sd <- c(0.05, 0.05, 0.05, 0.005)\n\n# mcmc loop\nfor (i in 1:n_iter) {\n  beta_proposal <- beta_current + rnorm(4, 0, proposal_sd)\n  log_post_proposal <- log_posterior(beta_proposal, df)\n\n  log_accept_ratio <- log_post_proposal - log_post_current\n  accept <- log(runif(1)) < log_accept_ratio\n\n  if (accept) {\n    beta_current <- beta_proposal\n    log_post_current <- log_post_proposal\n  }\n\n  beta_samples[i, ] <- beta_current\n}\n\n# discard burn-in\nposterior_draws <- beta_samples[(burn_in + 1):n_iter, ]\n\n# summarize\nposterior_summary <- apply(posterior_draws, 2, function(x) {\n  c(Mean = mean(x),\n    StdError = sd(x),\n    CI_Lower = quantile(x, 0.025),\n    CI_Upper = quantile(x, 0.975))\n})\nposterior_summary <- as.data.frame(t(posterior_summary))\n\nknitr::kable(posterior_summary,\n             caption = \"Posterior Means, Standard Deviations, and 95% Credible Intervals\")\n```\n\nTo visualize how the MCMC sampler behaved, we plot the trace and histogram for the posterior draws of \nβ\nprice\nβ \nprice\n​\t\n . The trace plot shows how the sampler explored the parameter space over time, while the histogram illustrates the shape and spread of the posterior distribution.\n \n```{r, warning=FALSE, message=FALSE}\n# extract posterior samples for beta_price\nbeta_price_draws <- posterior_draws[, \"price\"]\n\n# set up plotting area: 1 row, 2 plots\npar(mfrow = c(1, 2))\n\n# trace plot\nplot(beta_price_draws, type = \"l\", col = \"blue\",\n     main = \"Trace Plot of β_price\",\n     xlab = \"Iteration\", ylab = \"Value\")\n\n# histogram\nhist(beta_price_draws, breaks = 40, col = \"lightblue\", border = \"white\",\n     main = \"Posterior Distribution of β_price\",\n     xlab = \"β_price\")\n```\n\nThe table below compares the parameter estimates obtained using Maximum Likelihood and Bayesian (posterior) methods. For each of the four model parameters, we report the mean estimate, standard deviation (or standard error), and the corresponding 95% confidence or credible intervals. The similarity between the two sets of results indicates that the data were strong enough that the prior had little influence on the posterior.\n\n```{r, warning=FALSE, message=FALSE}\n# posterior summary\nposterior_summary <- apply(posterior_draws, 2, function(x) {\n  c(Mean = mean(x),\n    StdError = sd(x),\n    CI_Lower = quantile(x, 0.025),\n    CI_Upper = quantile(x, 0.975))\n})\nposterior_summary <- as.data.frame(t(posterior_summary))\n\n# mle summary\nstart_vals <- c(0, 0, 0, 0)\nmle_result <- optim(start_vals, log_likelihood, data = df, method = \"BFGS\", hessian = TRUE)\nbeta_hat <- mle_result$par\nvar_cov_matrix <- solve(mle_result$hessian)\nstd_errors <- sqrt(diag(var_cov_matrix))\nz <- qnorm(0.975)\nci_lower <- beta_hat - z * std_errors\nci_upper <- beta_hat + z * std_errors\n\nmle_summary <- data.frame(\n  mean = beta_hat,\n  sd = std_errors,\n  ci_lower = ci_lower,\n  ci_upper = ci_upper\n)\nrownames(mle_summary) <- c(\"brand_N\", \"brand_P\", \"ad_Yes\", \"price\")\n\n# combine: round only MLE values\ncomparison <- data.frame(\n  Parameter = rownames(mle_summary),\n  MLE_Mean = round(mle_summary$mean, 3),\n  MLE_SD = round(mle_summary$sd, 3),\n  MLE_Lower = round(mle_summary$ci_lower, 3),\n  MLE_Upper = round(mle_summary$ci_upper, 3),\n  Post_Mean = posterior_summary$Mean,\n  Post_SD = posterior_summary$StdError,\n  Post_Lower = posterior_summary$CI_Lower,\n  Post_Upper = posterior_summary$CI_Upper\n)\n\n# display table\nknitr::kable(comparison, caption = \"Comparison of MLE and Bayesian Estimates with 95% Intervals\")\n```\n\nThe results from the Bayesian estimation were very similar to those from the maximum likelihood approach. For all four parameters — brand_N, brand_P, ad_Yes, and price — the posterior means from the MCMC sampling were nearly identical to the MLE point estimates. The 95% credible intervals from the Bayesian method also closely matched the 95% confidence intervals from the MLE. For example, the estimated effect of price was –0.0995 under MLE and –0.100 under the Bayesian approach, with intervals that overlapped almost exactly. This consistency suggests that the data were strong and informative enough that the weakly informative priors had little effect on the posterior. The key advantage of the Bayesian method is that it provides full distributions for each parameter, which is especially useful for visualizing uncertainty and interpreting model results more flexibly.\n\n## 6. Discussion\n\n### Interpreting the Parameter Estimates\n\nEven if the data hadn’t been simulated, the parameter estimates make intuitive sense. The fact that $\\beta_\\text{Netflix} > \\beta_\\text{Prime}$ suggests that respondents preferred Netflix over Prime, all else equal. Since Hulu is the reference brand, both of these estimates are interpreted relative to it — meaning Netflix is the most preferred, Prime is somewhat preferred, and Hulu is the least preferred.\n\nThe negative sign on $\\beta_\\text{price}$ also aligns with expectations: as the monthly price increases, the likelihood of choosing that option decreases. This is consistent with typical consumer behavior — people tend to prefer cheaper options when everything else is held constant.\n  \n### Moving to a Hierarchical (Multi-Level) Model\n\nTo simulate data from a multi-level or hierarchical model (also called a random-parameter MNL), the main change would be allowing the $\\beta$ coefficients to vary across individuals, rather than assuming they are fixed for everyone. Instead of using a single set of part-worths that apply to all respondents, we assume that each person has their own vector of coefficients, $\\beta_i$, drawn from a population-level distribution — typically something like $\\beta_i \\sim \\mathcal{N}(\\mu, \\Sigma)$, where $\\mu$ is the average preference across respondents and $\\Sigma$ captures the variance and covariances in preferences.\n\nTo simulate this kind of data, we would draw a unique $\\beta_i$ for each respondent from the population distribution and then use that individual's $\\beta_i$ to generate their choices across tasks. Estimating this kind of model requires Bayesian methods (like hierarchical Bayes) or simulation-based techniques (like simulated maximum likelihood), since the model no longer has a closed-form likelihood. These models are more realistic for real-world conjoint data because they allow for heterogeneity in preferences — recognizing that not everyone values brand, ads, or price in the same way.\n\n\n\n\n\n\n\n\n","srcMarkdownNoYaml":"\n\nThis assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm. \n\n## 1. Likelihood for the Multi-nomial Logit (MNL) Model\n\nSuppose we have $i=1,\\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \\in \\{1, \\ldots, J\\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 3 products, then either $y=3$ or $y=(0,0,1)$ depending on how we want to represent it. Suppose also that we have a vector of data on each product $x_j$ (eg, brand, price, etc.). \n\nWe model the consumer's decision as the selection of the product that provides the most utility, and we'll specify the utility function as a linear function of the product characteristics:\n\n$$ U_{ij} = x_j'\\beta + \\epsilon_{ij} $$\n\nwhere $\\epsilon_{ij}$ is an i.i.d. extreme value error term. \n\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer $i$ chooses product $j$:\n\n$$ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} $$\n\nFor example, if there are 3 products, the probability that consumer $i$ chooses product 3 is:\n\n$$ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} $$\n\nA clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\\delta_{ij}$) that indicates the chosen product:\n\n$$ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}$$\n\nNotice that if the consumer selected product $j=3$, then $\\delta_{i3}=1$ while $\\delta_{i1}=\\delta_{i2}=0$ and the likelihood is:\n\n$$ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} $$\n\nThe joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:\n\n$$ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} $$\n\nAnd the joint log-likelihood function is:\n\n$$ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) $$\n\n\n\n## 2. Simulate Conjoint Data\n\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a \"no choice\" option; each simulated respondent must select one of the 3 alternatives. \n\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from \\$4 to \\$32 in increments of \\$4.\n\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer $i$ for hypothethical streaming service $j$ is \n\n$$\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n$$\n\nwhere the variables are binary indicators and $\\varepsilon$ is Type 1 Extreme Value (ie, Gumble) distributed.\n\nThe following code provides the simulation of the conjoint data based on the utility specification described above. Each respondent faces a series of choice tasks, and their selections are determined by both the systematic utility from product attributes and a random error term drawn from a Gumbel distribution. This setup mimics how real respondents might make decisions in a discrete choice experiment.\n\n:::: {.callout-note collapse=\"true\"}\n```{r}\n# set seed for reproducibility\nset.seed(123)\n\n# define attributes\nbrand <- c(\"N\", \"P\", \"H\") # Netflix, Prime, Hulu\nad <- c(\"Yes\", \"No\")\nprice <- seq(8, 32, by=4)\n\n# generate all possible profiles\nprofiles <- expand.grid(\n    brand = brand,\n    ad = ad,\n    price = price\n)\nm <- nrow(profiles)\n\n# assign part-worth utilities (true parameters)\nb_util <- c(N = 1.0, P = 0.5, H = 0)\na_util <- c(Yes = -0.8, No = 0.0)\np_util <- function(p) -0.1 * p\n\n# number of respondents, choice tasks, and alternatives per task\nn_peeps <- 100\nn_tasks <- 10\nn_alts <- 3\n\n# function to simulate one respondent’s data\nsim_one <- function(id) {\n  \n    datlist <- list()\n    \n    # loop over choice tasks\n    for (t in 1:n_tasks) {\n        \n        # randomly sample 3 alts (better practice would be to use a design)\n        dat <- cbind(resp=id, task=t, profiles[sample(m, size=n_alts), ])\n        \n        # compute deterministic portion of utility\n        dat$v <- b_util[dat$brand] + a_util[dat$ad] + p_util(dat$price) |> round(10)\n        \n        # add Gumbel noise (Type I extreme value)\n        dat$e <- -log(-log(runif(n_alts)))\n        dat$u <- dat$v + dat$e\n        \n        # identify chosen alternative\n        dat$choice <- as.integer(dat$u == max(dat$u))\n        \n        # store task\n        datlist[[t]] <- dat\n    }\n    \n    # combine all tasks for one respondent\n    do.call(rbind, datlist)\n}\n\n# simulate data for all respondents\nconjoint_data <- do.call(rbind, lapply(1:n_peeps, sim_one))\n\n# remove values unobservable to the researcher\nconjoint_data <- conjoint_data[ , c(\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\")]\n\n# clean up\nrm(list=setdiff(ls(), \"conjoint_data\"))\n```\n::::\n\n## 3. Preparing the Data for Estimation\n\nThe \"hard part\" of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer $i$, covariate $k$, and product $j$) instead of the typical 2 dimensions for cross-sectional regression models (consumer $i$ and covariate $k$). The fact that each task for each respondent has the same number of alternatives (3) helps.  In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\nIn order to estimate the MNL model, we need to reshape the data so that it includes one row per alternative, per choice task, per respondent. We also convert categorical variables (brand and ad format) into binary indicators using dummy coding, with Hulu and ad-free as the reference categories. This format allows us to calculate utility values and model choice behavior based on attribute-level part-worths.\n\n```{r, warning=FALSE, message=FALSE}\n# required libraries\nlibrary(dplyr)\nlibrary(tidyr)\n\n# work with simulated conjoint_data\ndf <- conjoint_data\n\n# convert to factors with reference levels\ndf$brand <- relevel(factor(df$brand), ref = \"H\")  # Hulu is reference\ndf$ad <- relevel(factor(df$ad), ref = \"No\")       # No ads is reference\n\n# create dummy variables, no intercept to avoid collinearity\nX <- model.matrix(~ brand + ad - 1, data = df)\nX <- as.data.frame(X)\n\n# rename to match previous convention\ncolnames(X) <- gsub(\"brandN\", \"brand_N\", colnames(X))\ncolnames(X) <- gsub(\"brandP\", \"brand_P\", colnames(X))\ncolnames(X) <- gsub(\"adYes\", \"ad_Yes\", colnames(X))  # after setting \"No\" as reference, this should now appear\n\n# combine and reorder\ndf <- bind_cols(df %>% select(resp, task, price, choice), X)\ndf <- df %>% select(resp, task, brand_N, brand_P, ad_Yes, price, choice)\n```\n\n## 4. Estimation via Maximum Likelihood\n\nTo estimate the parameters of the MNL model, we define a log-likelihood function that calculates the probability of each observed choice based on a given set of \nβ\nβ coefficients. The function computes the utility of each alternative, transforms it into a probability using the softmax formula, and sums the log-probabilities of the chosen alternatives. This function is then used in optimization to find the best-fitting parameters.\n\n```{r, warning=FALSE, message=FALSE}\n# log-likelihood function for MNL\nlog_likelihood <- function(beta, data) {\n  # unpack beta coefficients\n  b_brand_N <- beta[1]\n  b_brand_P <- beta[2]\n  b_ad_Yes  <- beta[3]\n  b_price   <- beta[4]\n  \n  # compute linear utility for all alternatives\n  data$v <- b_brand_N * data$brand_N +\n            b_brand_P * data$brand_P +\n            b_ad_Yes  * data$ad_Yes +\n            b_price   * data$price\n  \n  # normalize by task (3 alternatives per task)\n  data <- data %>% \n    group_by(resp, task) %>%\n    mutate(\n      exp_v = exp(v),\n      denom = sum(exp_v),\n      p = exp_v / denom,\n      logp = log(p)\n    ) %>%\n    ungroup()\n  \n  # compute total log-likelihood over chosen alternatives\n  ll <- sum(data$choice * data$logp)\n  return(-ll)  # return negative log-likelihood for minimization\n}\n```\n\nWith the log-likelihood function defined, we now use the optim() function in R to find the maximum likelihood estimates of the four model parameters. The optimization uses the BFGS method and returns both the coefficient estimates and the Hessian matrix. We use the inverse of the Hessian to calculate standard errors and construct 95% confidence intervals for each parameter.\n\n```{r, warning=FALSE, message=FALSE}\n# initial guess for the parameters\nstart_vals <- c(0, 0, 0, 0)  # brand_N, brand_P, ad_Yes, price\n\n# run optimization using BFGS (returns Hessian)\nmle_result <- optim(par = start_vals,\n                    fn = log_likelihood,\n                    data = df,\n                    method = \"BFGS\",\n                    hessian = TRUE)\n\n# extract estimated coefficients\nbeta_hat <- mle_result$par\n\n# compute standard errors from inverse Hessian\nvar_cov_matrix <- solve(mle_result$hessian)\nstd_errors <- sqrt(diag(var_cov_matrix))\n\n# 95% confidence intervals\nz_value <- qnorm(0.975)  # for 95% CI\nlower_ci <- beta_hat - z_value * std_errors\nupper_ci <- beta_hat + z_value * std_errors\n\n# build  summary table\nmle_summary <- data.frame(\n  Parameter = c(\"brand_N\", \"brand_P\", \"ad_Yes\", \"price\"),\n  Estimate = beta_hat,\n  StdError = std_errors,\n  CI_Lower = lower_ci,\n  CI_Upper = upper_ci\n)\n\nmle_summary <- mle_summary %>%\n  mutate(across(Estimate:CI_Upper))\n\nknitr::kable(mle_summary, caption = \"Maximum Likelihood Estimates with 95% Confidence Intervals\")\n```\n\n## 5. Estimation via Bayesian Methods\n\nWe now estimate the MNL model using a Bayesian approach via the Metropolis-Hastings MCMC algorithm. We define weakly informative priors for the coefficients and generate 11,000 samples from the posterior distribution, discarding the first 1,000 as burn-in. The log-likelihood function from the MLE step is reused, and posterior draws are accepted or rejected based on the Metropolis-Hastings criterion. This approach gives us a full distribution for each parameter rather than just point estimates.\n\n```{r, warning=FALSE, message=FALSE}\n# log prior\nlog_prior <- function(beta) {\n  # brand_N, brand_P, ad_Yes ~ N(0, 5); price ~ N(0, 1)\n  lp <- dnorm(beta[1], 0, sqrt(5), log = TRUE) +\n        dnorm(beta[2], 0, sqrt(5), log = TRUE) +\n        dnorm(beta[3], 0, sqrt(5), log = TRUE) +\n        dnorm(beta[4], 0, 1, log = TRUE)\n  return(lp)\n}\n\n# log posterior = log-likelihood + log-prior\nlog_posterior <- function(beta, data) {\n  -log_likelihood(beta, data) + log_prior(beta)\n}\n\n# mcmc settings\nn_iter <- 11000\nburn_in <- 1000\nkeep <- n_iter - burn_in\n\n# storage\nbeta_samples <- matrix(NA, nrow = n_iter, ncol = 4)\ncolnames(beta_samples) <- c(\"brand_N\", \"brand_P\", \"ad_Yes\", \"price\")\n\n# initialize\nbeta_current <- c(0, 0, 0, 0)\nlog_post_current <- log_posterior(beta_current, df)\n\n# proposal sd (step size)\nproposal_sd <- c(0.05, 0.05, 0.05, 0.005)\n\n# mcmc loop\nfor (i in 1:n_iter) {\n  beta_proposal <- beta_current + rnorm(4, 0, proposal_sd)\n  log_post_proposal <- log_posterior(beta_proposal, df)\n\n  log_accept_ratio <- log_post_proposal - log_post_current\n  accept <- log(runif(1)) < log_accept_ratio\n\n  if (accept) {\n    beta_current <- beta_proposal\n    log_post_current <- log_post_proposal\n  }\n\n  beta_samples[i, ] <- beta_current\n}\n\n# discard burn-in\nposterior_draws <- beta_samples[(burn_in + 1):n_iter, ]\n\n# summarize\nposterior_summary <- apply(posterior_draws, 2, function(x) {\n  c(Mean = mean(x),\n    StdError = sd(x),\n    CI_Lower = quantile(x, 0.025),\n    CI_Upper = quantile(x, 0.975))\n})\nposterior_summary <- as.data.frame(t(posterior_summary))\n\nknitr::kable(posterior_summary,\n             caption = \"Posterior Means, Standard Deviations, and 95% Credible Intervals\")\n```\n\nTo visualize how the MCMC sampler behaved, we plot the trace and histogram for the posterior draws of \nβ\nprice\nβ \nprice\n​\t\n . The trace plot shows how the sampler explored the parameter space over time, while the histogram illustrates the shape and spread of the posterior distribution.\n \n```{r, warning=FALSE, message=FALSE}\n# extract posterior samples for beta_price\nbeta_price_draws <- posterior_draws[, \"price\"]\n\n# set up plotting area: 1 row, 2 plots\npar(mfrow = c(1, 2))\n\n# trace plot\nplot(beta_price_draws, type = \"l\", col = \"blue\",\n     main = \"Trace Plot of β_price\",\n     xlab = \"Iteration\", ylab = \"Value\")\n\n# histogram\nhist(beta_price_draws, breaks = 40, col = \"lightblue\", border = \"white\",\n     main = \"Posterior Distribution of β_price\",\n     xlab = \"β_price\")\n```\n\nThe table below compares the parameter estimates obtained using Maximum Likelihood and Bayesian (posterior) methods. For each of the four model parameters, we report the mean estimate, standard deviation (or standard error), and the corresponding 95% confidence or credible intervals. The similarity between the two sets of results indicates that the data were strong enough that the prior had little influence on the posterior.\n\n```{r, warning=FALSE, message=FALSE}\n# posterior summary\nposterior_summary <- apply(posterior_draws, 2, function(x) {\n  c(Mean = mean(x),\n    StdError = sd(x),\n    CI_Lower = quantile(x, 0.025),\n    CI_Upper = quantile(x, 0.975))\n})\nposterior_summary <- as.data.frame(t(posterior_summary))\n\n# mle summary\nstart_vals <- c(0, 0, 0, 0)\nmle_result <- optim(start_vals, log_likelihood, data = df, method = \"BFGS\", hessian = TRUE)\nbeta_hat <- mle_result$par\nvar_cov_matrix <- solve(mle_result$hessian)\nstd_errors <- sqrt(diag(var_cov_matrix))\nz <- qnorm(0.975)\nci_lower <- beta_hat - z * std_errors\nci_upper <- beta_hat + z * std_errors\n\nmle_summary <- data.frame(\n  mean = beta_hat,\n  sd = std_errors,\n  ci_lower = ci_lower,\n  ci_upper = ci_upper\n)\nrownames(mle_summary) <- c(\"brand_N\", \"brand_P\", \"ad_Yes\", \"price\")\n\n# combine: round only MLE values\ncomparison <- data.frame(\n  Parameter = rownames(mle_summary),\n  MLE_Mean = round(mle_summary$mean, 3),\n  MLE_SD = round(mle_summary$sd, 3),\n  MLE_Lower = round(mle_summary$ci_lower, 3),\n  MLE_Upper = round(mle_summary$ci_upper, 3),\n  Post_Mean = posterior_summary$Mean,\n  Post_SD = posterior_summary$StdError,\n  Post_Lower = posterior_summary$CI_Lower,\n  Post_Upper = posterior_summary$CI_Upper\n)\n\n# display table\nknitr::kable(comparison, caption = \"Comparison of MLE and Bayesian Estimates with 95% Intervals\")\n```\n\nThe results from the Bayesian estimation were very similar to those from the maximum likelihood approach. For all four parameters — brand_N, brand_P, ad_Yes, and price — the posterior means from the MCMC sampling were nearly identical to the MLE point estimates. The 95% credible intervals from the Bayesian method also closely matched the 95% confidence intervals from the MLE. For example, the estimated effect of price was –0.0995 under MLE and –0.100 under the Bayesian approach, with intervals that overlapped almost exactly. This consistency suggests that the data were strong and informative enough that the weakly informative priors had little effect on the posterior. The key advantage of the Bayesian method is that it provides full distributions for each parameter, which is especially useful for visualizing uncertainty and interpreting model results more flexibly.\n\n## 6. Discussion\n\n### Interpreting the Parameter Estimates\n\nEven if the data hadn’t been simulated, the parameter estimates make intuitive sense. The fact that $\\beta_\\text{Netflix} > \\beta_\\text{Prime}$ suggests that respondents preferred Netflix over Prime, all else equal. Since Hulu is the reference brand, both of these estimates are interpreted relative to it — meaning Netflix is the most preferred, Prime is somewhat preferred, and Hulu is the least preferred.\n\nThe negative sign on $\\beta_\\text{price}$ also aligns with expectations: as the monthly price increases, the likelihood of choosing that option decreases. This is consistent with typical consumer behavior — people tend to prefer cheaper options when everything else is held constant.\n  \n### Moving to a Hierarchical (Multi-Level) Model\n\nTo simulate data from a multi-level or hierarchical model (also called a random-parameter MNL), the main change would be allowing the $\\beta$ coefficients to vary across individuals, rather than assuming they are fixed for everyone. Instead of using a single set of part-worths that apply to all respondents, we assume that each person has their own vector of coefficients, $\\beta_i$, drawn from a population-level distribution — typically something like $\\beta_i \\sim \\mathcal{N}(\\mu, \\Sigma)$, where $\\mu$ is the average preference across respondents and $\\Sigma$ captures the variance and covariances in preferences.\n\nTo simulate this kind of data, we would draw a unique $\\beta_i$ for each respondent from the population distribution and then use that individual's $\\beta_i$ to generate their choices across tasks. Estimating this kind of model requires Bayesian methods (like hierarchical Bayes) or simulation-based techniques (like simulated maximum likelihood), since the model no longer has a closed-form likelihood. These models are more realistic for real-world conjoint data because they allow for heterogeneity in preferences — recognizing that not everyone values brand, ads, or price in the same way.\n\n\n\n\n\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"cosmo","title":"Multinomial Logit Model","author":"Charlotte Hunter","date":"today","callout-appearance":"minimal"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}