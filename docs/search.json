[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Charlotte Hunter",
    "section": "",
    "text": "Quarto website: https://quarto.org/docs/websites"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "My Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nCharlotte Hunter\n\n\nMay 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Model\n\n\n\n\n\n\nCharlotte Hunter\n\n\nMay 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nCharlotte Hunter\n\n\nMay 28, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/project1/index.html",
    "href": "blog/project1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn the experiment, Karlan and List partnered with a nonprofit organization to explore how different types of donation appeals affect giving behavior. While all recipients were previous donors, the letters varied in key ways: some offered no incentive, while others included matching grants of $1:$1, $2:$1, or $3:$1, where a “leadership donor” pledged to match donations up to a certain amount. The letters also varied the suggested contribution amount and the cap on how much the leadership donor would match.\nBy randomly assigning these conditions and tracking donations in response, the researchers were able to estimate how changes in perceived “price” and framing influenced both the probability of donating and the total amount given. This project seeks to replicate their findings using the original data and statistical approach.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/index.html#sub-header",
    "href": "blog/project1/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "blog/project2/index.html",
    "href": "blog/project2/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\nCode\n# histogram of patents\nggplot(blueprinty, aes(x = patents, fill = factor(iscustomer))) +\n  geom_histogram(binwidth = 1, position = \"dodge\", alpha = 0.7) +\n  labs(\n    title = \"Histogram of Patents by Customer Status\",\n    x = \"Number of Patents\",\n    y = \"Count\",\n    fill = \"Customer Status\"\n  ) +\n  scale_fill_manual(values = c(\"0\" = \"gray\", \"1\" = \"steelblue\"),\n                    labels = c(\"0\" = \"Non-Customer\", \"1\" = \"Customer\"))\n\n\n\n\n\n\n\nCode\n# average patents table\nblueprinty %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarize(mean_patents = mean(patents, na.rm = TRUE)) %&gt;%\n  mutate(\n    iscustomer = ifelse(iscustomer == 1, \"Customer\", \"Non-Customer\")\n  ) %&gt;%\n  kable(digits = 2, col.names = c(\"Customer Status\", \"Average Number of Patents\"),\n        caption = \"Average Patents by Customer Status\")\n\n\n\nAverage Patents by Customer Status\n\n\nCustomer Status\nAverage Number of Patents\n\n\n\n\nNon-Customer\n3.47\n\n\nCustomer\n4.13\n\n\n\n\n\nCustomers tend to have a higher number of patents than non-customers. The average for customers is 4.13 patents, compared to 3.47 for non-customers. The histogram shows that customers are more likely to have higher patent counts, with their distribution shifted to the right. Non-customers are more concentrated in the 0–3 patent range, while customers are more represented in the 4–8 range. This suggests a positive relationship between patent ownership and the likelihood of being a customer.\n\n\nCode\n# boxplot age distribution\nggplot(blueprinty, aes(x = factor(iscustomer), y = age, fill = factor(iscustomer))) +\n  geom_boxplot() +\n  labs(\n    title = \"Age Distribution by Customer Status\",\n    x = \"Customer Status\",\n    y = \"Age\",\n    fill = \"Customer\"\n  ) +\n  scale_fill_manual(values = c(\"0\" = \"gray\", \"1\" = \"steelblue\"),\n                    labels = c(\"0\" = \"Non-Customer\", \"1\" = \"Customer\"))\n\n\n\n\n\n\n\nCode\n# mean age table\nblueprinty %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarize(mean_age = mean(age, na.rm = TRUE)) %&gt;%\n  mutate(\n    iscustomer = ifelse(iscustomer == 1, \"Customer\", \"Non-Customer\")\n  ) %&gt;%\n  kable(digits = 2, col.names = c(\"Customer Status\", \"Average Age\"),\n        caption = \"Average Age by Customer Status\")\n\n\n\nAverage Age by Customer Status\n\n\nCustomer Status\nAverage Age\n\n\n\n\nNon-Customer\n26.1\n\n\nCustomer\n26.9\n\n\n\n\n\nCustomers are slightly older than non-customers on average (26.9 vs. 26.1 years). The boxplot shows similar distributions, but customers have a slightly higher median and a longer upper tail. This suggests that customer status is somewhat associated with age, though the difference is modest.\n\n\nCode\n# proportions for each region by customer status\nregion_customer_prop &lt;- blueprinty %&gt;%\n  group_by(region, iscustomer) %&gt;%\n  summarise(count = n(), .groups = \"drop\") %&gt;%\n  group_by(region) %&gt;%\n  mutate(prop = count / sum(count)) %&gt;%\n  ungroup() %&gt;%\n  mutate(iscustomer = factor(iscustomer, labels = c(\"Non-Customer\", \"Customer\")))\n\n# bar plot \nggplot(region_customer_prop, aes(x = region, y = prop, fill = iscustomer)) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.7)) +\n  labs(\n    title = \"Proportion of Customers and Non-Customers by Region\",\n    x = \"Region\",\n    y = \"Proportion\",\n    fill = \"Customer Status\"\n  ) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme_minimal()\n\n\n\n\n\nRegional differences are more pronounced. In the Northeast, over half of the firms are customers — a much higher proportion than any other region. In contrast, regions like the Midwest, Northwest, South, and Southwest have much lower customer proportions (generally below 20%). This suggests strong regional variation in customer conversion, with the Northeast standing out as Blueprinty’s most successful market.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nAssume we have ( n ) independent observations ( Y_1, Y_2, , Y_n ), each distributed as:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda)\n\\]\nThe probability mass function is:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nSince the observations are independent, the likelihood function is the product of their individual densities:\n\\[\n\\mathcal{L}(\\lambda \\mid Y_1, \\dots, Y_n) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nThis simplifies to:\n\\[\n\\mathcal{L}(\\lambda) = \\frac{e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n Y_i}}{\\prod_{i=1}^n Y_i!}\n\\]\n\n\nCode\n# log-likelihood function for Poisson model\npoisson_loglikelihood &lt;- function(lambda, Y) {\n  if (lambda &lt;= 0) return(-Inf)  # Invalid for log\n  n &lt;- length(Y)\n  ll &lt;- -n * lambda + sum(Y) * log(lambda) - sum(lgamma(Y + 1))\n  return(ll)\n}\n\n\n\n\nCode\n# use the observed number \ny_data &lt;- blueprinty$patents\n\n# define a sequence of lambda values\nlambda_vals &lt;- seq(0.1, 10, length.out = 200)\n\n# log-likelihoods\nlog_likelihoods &lt;- sapply(lambda_vals, function(l) poisson_loglikelihood(l, y_data))\n\n# plot\nplot(lambda_vals, log_likelihoods, type = \"l\",\n     main = \"Log-Likelihood of Poisson Model\",\n     xlab = expression(lambda),\n     ylab = \"Log-Likelihood\",\n     col = \"steelblue\", lwd = 2)\n\n\n\n\n\nWe start with the log-likelihood function for ( n ) independent observations from a Poisson distribution:\n\\[\n\\log \\mathcal{L}(\\lambda) = -n\\lambda + \\left(\\sum_{i=1}^n Y_i\\right) \\log \\lambda - \\sum_{i=1}^n \\log(Y_i!)\n\\]\nTaking the derivative with respect to ( ):\n\\[\n\\frac{d}{d\\lambda} \\log \\mathcal{L}(\\lambda) = -n + \\frac{\\sum_{i=1}^n Y_i}{\\lambda}\n\\]\nSetting the derivative equal to zero:\n\\[\n-n + \\frac{\\sum_{i=1}^n Y_i}{\\lambda} = 0\n\\quad \\Rightarrow \\quad\n\\hat{\\lambda} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n\\]\nThis result makes intuitive sense, as the mean of a Poisson distribution is (), so the MLE is simply the sample mean.\n\n\nCode\n# negative log-likelihood wrapper for use with optim()\nneg_log_likelihood_poisson &lt;- function(lambda, Y) {\n  if (lambda &lt;= 0) return(Inf)\n  -poisson_loglikelihood(lambda, Y)\n}\n\n# use optim to find MLE\noptim_result &lt;- optim(\n  par = 1,  # initial guess\n  fn = neg_log_likelihood_poisson,\n  Y = blueprinty$patents,\n  method = \"Brent\",\n  lower = 0.001,\n  upper = 10\n)\n\n# extract MLE estimate\nlambda_mle_optim &lt;- optim_result$par\ncat(\"MLE estimate of lambda:\", round(lambda_mle_optim, 3), \"\\n\")\n\n\nMLE estimate of lambda: 3.685 \n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\nCode\n# log-likelihood for Poisson regression\npoisson_regression_loglik &lt;- function(beta, Y, X) {\n  eta &lt;- X %*% beta                    \n  lambda &lt;- exp(eta)                  \n  ll &lt;- sum(Y * log(lambda) - lambda - lgamma(Y + 1))\n  return(ll)\n}\n\n\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\n\n\nCode\nblueprinty$age2 &lt;- blueprinty$age^2\n\n# region dummy variables, dropping one region\nregion_dummies &lt;- model.matrix(~ region, data = blueprinty)[, -1]  # drops intercept & first level\n\n# matrix\nX &lt;- cbind(Intercept = 1,\n           age = blueprinty$age,\n           age2 = blueprinty$age2,\n           region_dummies,\n           iscustomer = blueprinty$iscustomer)\n\nY &lt;- blueprinty$patents\n\n# define log-likelihood function\nneg_loglik &lt;- function(beta, Y, X) {\n  eta &lt;- X %*% beta\n  lambda &lt;- exp(eta)\n  -sum(Y * log(lambda) - lambda - lgamma(Y + 1))  # negative\n}\n\n# optimize using optim()\ninit_beta &lt;- rep(0, ncol(X))  \nfit &lt;- optim(\n  par = init_beta,\n  fn = neg_loglik,\n  Y = Y,\n  X = X,\n  hessian = TRUE,\n  method = \"BFGS\"\n)\n\n# extract coefficients & standard errors\nbeta_hat &lt;- fit$par\nhessian &lt;- fit$hessian\nse_beta &lt;- sqrt(diag(solve(hessian)))\n\n# fix possible duplicate column names\ncolnames_X &lt;- make.names(colnames(X), unique = TRUE)\n\n# create & display coefficient table\ncoef_table &lt;- data.frame(\n  Term = colnames_X,\n  Coefficient = beta_hat,\n  Std_Error = se_beta\n)\n\nknitr::kable(coef_table, digits = 3, caption = \"Poisson Regression Coefficients and Standard Errors\")\n\n\n\nPoisson Regression Coefficients and Standard Errors\n\n\nTerm\nCoefficient\nStd_Error\n\n\n\n\nIntercept\n-0.126\n0.112\n\n\nage\n0.116\n0.006\n\n\nage2\n-0.002\n0.000\n\n\nregionNortheast\n-0.025\n0.043\n\n\nregionNorthwest\n-0.035\n0.053\n\n\nregionSouth\n-0.005\n0.052\n\n\nregionSouthwest\n-0.038\n0.047\n\n\niscustomer\n0.061\n0.032\n\n\n\n\n\n\n\nCode\n# fit Poisson regression using glm()\nglm_fit &lt;- glm(\n  patents ~ age + I(age^2) + region + iscustomer,\n  data = blueprinty,\n  family = poisson(link = \"log\")\n)\n\nsummary(glm_fit)\n\n\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + iscustomer, \n    family = poisson(link = \"log\"), data = blueprinty)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.508920   0.183179  -2.778  0.00546 ** \nage              0.148619   0.013869  10.716  &lt; 2e-16 ***\nI(age^2)        -0.002971   0.000258 -11.513  &lt; 2e-16 ***\nregionNortheast  0.029170   0.043625   0.669  0.50372    \nregionNorthwest -0.017574   0.053781  -0.327  0.74383    \nregionSouth      0.056561   0.052662   1.074  0.28281    \nregionSouthwest  0.050576   0.047198   1.072  0.28391    \niscustomer       0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n\n\nThe Poisson regression results show that age and customer status are strong predictors of patent counts. Age has a positive effect, but the negative coefficient on age² indicates diminishing returns, suggesting patent activity peaks at a certain age. Being a customer increases expected patent counts by approximately 23%, holding other factors constant. Regional effects are not statistically significant, indicating that after adjusting for age and customer status, geography does not explain substantial variation in patent activity.\n\n\n\n\n\n\nPoisson Regression Summary\n\n\n\n\nAge: Significant positive effect with diminishing returns (non-linear curve due to age²)\nCustomer status: Customers have ~23% more patents than non-customers (exp(0.208))\nRegion: No region has a statistically significant effect on patent counts\nModel insight: Patent activity is best explained by age (with a peak) and customer conversion\n\n\n\n\n\nCode\nX_base &lt;- cbind(\n  Intercept = 1,\n  age = blueprinty$age,\n  age2 = blueprinty$age^2,\n  model.matrix(~ region, data = blueprinty)[, -1],  # region dummies (drop intercept)\n  iscustomer = blueprinty$iscustomer\n)\n\n# create datasets\nX_0 &lt;- X_base\nX_0[, \"iscustomer\"] &lt;- 0\n\nX_1 &lt;- X_base\nX_1[, \"iscustomer\"] &lt;- 1\n\n# use beta_hat (from optim) \neta_0 &lt;- X_0 %*% beta_hat\neta_1 &lt;- X_1 %*% beta_hat\n\ny_pred_0 &lt;- exp(eta_0)\ny_pred_1 &lt;- exp(eta_1)\n\n# average difference \navg_diff &lt;- mean(y_pred_1 - y_pred_0)\ncat(\"Estimated average increase:\", round(avg_diff, 3), \"\\n\")\n\n\nEstimated average increase: 0.218"
  },
  {
    "objectID": "blog/project2/index.html#sub-header",
    "href": "blog/project2/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "blog/project1/index.html#section-1-data",
    "href": "blog/project1/index.html#section-1-data",
    "title": "This is project 1",
    "section": "",
    "text": "I cleaned some data"
  },
  {
    "objectID": "blog/project1/index.html#section-2-analysis",
    "href": "blog/project1/index.html#section-2-analysis",
    "title": "This is project 1",
    "section": "",
    "text": "I analyzed some data\n\nlibrary(tidyverse)\nmtcars |&gt; \n  ggplot(aes(x = wt, y = mpg)) + geom_point()"
  },
  {
    "objectID": "blog/project1/hw1_questions.html",
    "href": "blog/project1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn the experiment, Karlan and List partnered with a nonprofit organization to explore how different types of donation appeals affect giving behavior. While all recipients were previous donors, the letters varied in key ways: some offered no incentive, while others included matching grants of $1:$1, $2:$1, or $3:$1, where a “leadership donor” pledged to match donations up to a certain amount. The letters also varied the suggested contribution amount and the cap on how much the leadership donor would match.\nBy randomly assigning these conditions and tracking donations in response, the researchers were able to estimate how changes in perceived “price” and framing influenced both the probability of donating and the total amount given. This project seeks to replicate their findings using the original data and statistical approach.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#introduction",
    "href": "blog/project1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn the experiment, Karlan and List partnered with a nonprofit organization to explore how different types of donation appeals affect giving behavior. While all recipients were previous donors, the letters varied in key ways: some offered no incentive, while others included matching grants of $1:$1, $2:$1, or $3:$1, where a “leadership donor” pledged to match donations up to a certain amount. The letters also varied the suggested contribution amount and the cap on how much the leadership donor would match.\nBy randomly assigning these conditions and tracking donations in response, the researchers were able to estimate how changes in perceived “price” and framing influenced both the probability of donating and the total amount given. This project seeks to replicate their findings using the original data and statistical approach.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#data",
    "href": "blog/project1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nThe dataset karlan_list_2007.dta captures the results of a large-scale field experiment on charitable giving, involving 50,083 previous donors to a nonprofit organization. Each row in the dataset represents one individual who received a fundraising letter during the campaign. The experimental design randomly assigned participants to a control group or to one of several treatment groups, which varied along three key dimensions: the match ratio offered (1:1, 2:1, or 3:1), the maximum size of the matching grant ($25,000, $50,000, $100,000, or unstated), and the suggested donation amount (based on prior giving history, scaled by 1.0, 1.25, or 1.5). The dataset records whether the individual made a donation (gave) and how much they donated (amount). It also contains detailed prior giving history, including the number of past donations, recency of giving, and highest previous contribution. Demographic information is included at the zip-code level, such as racial composition (pwhite, pblack), median household income, average household size, homeownership rates, and educational attainment. Additionally, political context is captured through indicators for whether the individual lived in a red or blue state or county during the 2004 presidential election. This rich set of variables enables analysis not only of overall treatment effects but also of heterogeneous effects across political and demographic subgroups.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nnames(df)\n\n\n\n\n### Linear Regression: mrm2 ~ treatment\n\n\n             Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept) 12.998142   0.093526 138.9789   &lt;2e-16 ***\ntreatment    0.013686   0.114534   0.1195   0.9049    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n### Linear Regression: female ~ treatment\n\n\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.2826978  0.0035036 80.6879  &lt; 2e-16 ***\ntreatment   -0.0075469  0.0042920 -1.7584  0.07869 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n### Linear Regression: years ~ treatment\n\n\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.135914   0.042604 144.023   &lt;2e-16 ***\ntreatment   -0.057549   0.052173  -1.103     0.27    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTo test whether the treatment and control groups were balanced before the experiment began, I ran both a t-test and a linear regression using the variable mrm2, which represents months since last donation. The t-test yielded a t-statistic of approximately 0.12, indicating no statistically significant difference between the groups. I then ran a linear regression of mrm2 on the treatment variable. The coefficient on treatment was nearly zero and not statistically significant, confirming the same result: the groups were balanced. This matches the purpose of Table 1 in the paper, which is to verify that the randomization was successful and that the treatment groups were similar before the intervention. This helps isolate the causal effect of the fundraising treatments observed later in the study."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#experimental-results",
    "href": "blog/project1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n\n\n\n\n\nThis bar plot shows the proportion of people who donated in each group. The treatment group, which received a fundraising letter mentioning a matching donation, had a slightly higher donation rate than the control group. While both rates are low overall, the visual difference suggests that even a small change in how the request was framed — in this case, offering a match — may have encouraged more people to give.\n\n\nManual T-Test: gave ~ treatment Calculated t-statistic: 3.2095\nLinear Regression: gave ~ treatment\n             Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 0.0178582  0.0011007 16.2246 &lt; 2.2e-16 ***\ntreatment   0.0041804  0.0013479  3.1014  0.001927 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nI compared the proportion of people who donated in the treatment and control groups. In the control group, about 1.8% of individuals donated, compared to 2.2% in the treatment group. Although this seems like a small difference, a statistical test showed that it is highly unlikely to have occurred by chance. I confirmed this result using both a t-test (following the class formula) and a simple linear regression. Both methods showed the same result — that people who received a matching grant letter were significantly more likely to give.\nThis tells us something important about human behavior: even a small change in how a request is framed — like offering to match someone’s donation — can meaningfully affect whether they decide to act. The offer likely made giving feel more impactful or urgent. This kind of result is exactly what Table 2A in the paper is highlighting: how relatively subtle changes in message framing can lead to statistically and practically meaningful behavior change.\n\n\n\n\n\nCall:\nglm(formula = gave ~ treatment, family = binomial(link = \"probit\"), \n    data = df)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.10014    0.02332 -90.074  &lt; 2e-16 ***\ntreatment    0.08678    0.02788   3.113  0.00185 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 10071  on 50082  degrees of freedom\nResidual deviance: 10061  on 50081  degrees of freedom\nAIC: 10065\n\nNumber of Fisher Scoring iterations: 6\n\n\nI ran a probit regression where the outcome variable was whether an individual donated (gave) and the explanatory variable was whether they were assigned to a treatment group (treatment). The coefficient on treatment was small but positive and statistically significant, replicating the result shown in Table 3, Column 1 of the Karlan & List paper. This suggests that receiving a letter with a matching grant offer had a statistically significant effect on the probability of donating, even after accounting for the binary nature of the outcome variable using a probit model.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n\n\n\n    Welch Two Sample t-test\n\ndata:  gave by ratio\nt = -0.96505, df = 22225, p-value = 0.3345\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -0.005711275  0.001942773\nsample estimates:\nmean in group 1 mean in group 2 \n     0.02074912      0.02263338 \n\n\n\n    Welch Two Sample t-test\n\ndata:  gave by ratio\nt = -1.015, df = 22215, p-value = 0.3101\nalternative hypothesis: true difference in means between group 1 and group 3 is not equal to 0\n95 percent confidence interval:\n -0.005816051  0.001847501\nsample estimates:\nmean in group 1 mean in group 3 \n     0.02074912      0.02273340 \n\n\n\n    Welch Two Sample t-test\n\ndata:  gave by ratio\nt = -0.050116, df = 22261, p-value = 0.96\nalternative hypothesis: true difference in means between group 2 and group 3 is not equal to 0\n95 percent confidence interval:\n -0.004012044  0.003811996\nsample estimates:\nmean in group 2 mean in group 3 \n     0.02263338      0.02273340 \n\n\nTo test whether larger match ratios increase the likelihood of donating, I ran a series of t-tests comparing donation rates between the 1:1, 2:1, and 3:1 match groups. The results showed no statistically significant differences. The 2:1 and 3:1 groups had slightly higher donation rates than the 1:1 group (about 2.27% vs. 2.07%), but these differences were small and not statistically meaningful (p-values &gt; 0.30). A comparison between the 2:1 and 3:1 groups yielded almost identical results, with a p-value of 0.96. These findings support the authors’ claim on page 8 that increasing the match ratio beyond 1:1 does not further increase giving. This suggests that the presence of a match may be what matters most — once donors see their contribution will be matched, making the match larger does not add much persuasive power.\n\n\n\n\nCall:\nlm(formula = gave ~ ratio, data = match_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.02273 -0.02273 -0.02263 -0.02075  0.97925 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.020749   0.001391  14.912   &lt;2e-16 ***\nratio2      0.001884   0.001968   0.958    0.338    \nratio3      0.001984   0.001968   1.008    0.313    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1468 on 33393 degrees of freedom\nMultiple R-squared:  3.865e-05, Adjusted R-squared:  -2.124e-05 \nF-statistic: 0.6454 on 2 and 33393 DF,  p-value: 0.5245\n\n\nIn the regression model comparing match ratios, the 1:1 match group had a baseline donation rate of approximately 2.07%. The 2:1 and 3:1 match groups showed slightly higher estimated donation rates—about 0.19 and 0.20 percentage points more, respectively—but these differences were not statistically significant. Both coefficients had relatively large standard errors and high p-values (above 0.3), indicating low statistical precision. This means we cannot confidently say that higher match ratios had any real effect beyond the 1:1 match. These results support the conclusion that increasing the match ratio does not significantly influence the likelihood of donating.\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n\n\n# A tibble: 3 × 2\n  ratio response_rate\n  &lt;fct&gt;         &lt;dbl&gt;\n1 1            0.0207\n2 2            0.0226\n3 3            0.0227\n\n\n\nCall:\nlm(formula = gave ~ ratio, data = match_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.02273 -0.02273 -0.02263 -0.02075  0.97925 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.020749   0.001391  14.912   &lt;2e-16 ***\nratio2      0.001884   0.001968   0.958    0.338    \nratio3      0.001984   0.001968   1.008    0.313    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1468 on 33393 degrees of freedom\nMultiple R-squared:  3.865e-05, Adjusted R-squared:  -2.124e-05 \nF-statistic: 0.6454 on 2 and 33393 DF,  p-value: 0.5245\n\n\nI calculated the difference in response rates between the 1:1 and 2:1 match groups, and between the 2:1 and 3:1 groups, both directly from the data and using the coefficients from a linear regression. In both cases, the differences were very small—around 0.19 percentage points for 2:1 vs. 1:1, and essentially zero for 3:1 vs. 2:1. These results suggest that increasing the size of the matching donation beyond 1:1 has minimal to no effect on whether people choose to give. The consistency between the raw data and regression confirms that larger match ratios do not meaningfully improve donation rates, reinforcing the idea that simply offering a match may matter more than how generous the match actually is.\n\n\n\n\nCall:\nlm(formula = amount ~ ratio, data = donors_only)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-44.34 -25.14 -16.25   4.86 354.86 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  45.1429     2.7654  16.324   &lt;2e-16 ***\nratio2        0.1944     3.8285   0.051    0.960    \nratio3       -3.8911     3.8249  -1.017    0.309    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 42.03 on 733 degrees of freedom\nMultiple R-squared:  0.002043,  Adjusted R-squared:  -0.0006797 \nF-statistic: 0.7504 on 2 and 733 DF,  p-value: 0.4726\n\n\nI ran a regression to examine whether the size of the match ratio influenced how much people donated, among those who chose to give. The results showed no statistically significant differences between the 1:1, 2:1, and 3:1 groups. Donors in the 2:1 group gave about 19 cents more than the 1:1 group, and donors in the 3:1 group gave about $3.89 less, but neither difference was meaningful or statistically reliable. However, this regression does not have a valid causal interpretation, since it only includes people who donated — a post-treatment outcome that breaks the randomization. As a result, we cannot conclude whether the match ratio itself caused any differences in donation amounts."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#simulation-experiment",
    "href": "blog/project1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\n\n\n\n\n\nThis plot shows the cumulative average of differences in donation amounts between the treatment and control groups, based on simulated data. I drew 10,000 values from the treatment group distribution and compared each to a randomly drawn value from the control group distribution. Each point on the blue line represents the average difference after one more simulated pair. As expected, the early differences are highly variable, but the line stabilizes as more data accumulate. The red dashed line represents the actual difference in means from the real data. The fact that the blue line converges to this value illustrates a key concept in statistics: with enough data, sampling variation averages out, and the sample-based estimate approaches the true effect.\n\n\nCentral Limit Theorem\n\n\n\n\n\n\nThese histograms show the distribution of estimated treatment effects across 1,000 simulations for sample sizes of 50, 200, 500, and 1000. In each simulation, I drew independent samples from the treatment and control distributions and computed the difference in their means. At smaller sample sizes, the distributions are wider and noisier, with the red line at zero appearing near the center. This means that with a small sample, it’s quite common to estimate a treatment effect close to zero just by chance — even if a true effect exists. As the sample size increases, the distribution of estimates becomes narrower, and zero shifts toward the tail, particularly in the histogram for sample size 1000. This indicates that with larger samples, it’s much less likely for random variation to produce a near-zero estimate when there is a true effect. Overall, the plots illustrate how larger samples reduce sampling variability, increase precision, and improve our ability to detect real treatment effects, especially when those effects are small."
  },
  {
    "objectID": "blog/project1/index.html#introduction",
    "href": "blog/project1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn the experiment, Karlan and List partnered with a nonprofit organization to explore how different types of donation appeals affect giving behavior. While all recipients were previous donors, the letters varied in key ways: some offered no incentive, while others included matching grants of $1:$1, $2:$1, or $3:$1, where a “leadership donor” pledged to match donations up to a certain amount. The letters also varied the suggested contribution amount and the cap on how much the leadership donor would match.\nBy randomly assigning these conditions and tracking donations in response, the researchers were able to estimate how changes in perceived “price” and framing influenced both the probability of donating and the total amount given. This project seeks to replicate their findings using the original data and statistical approach.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/index.html#data",
    "href": "blog/project1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\n# Load libraries\nlibrary(haven)\nlibrary(dplyr)\n\ndf &lt;- read_dta(\"~/Desktop/karlan_list_2007.dta\")\n\nThe dataset karlan_list_2007.dta captures the results of a large-scale field experiment on charitable giving, involving 50,083 previous donors to a nonprofit organization. Each row in the dataset represents one individual who received a fundraising letter during the campaign. The experimental design randomly assigned participants to a control group or to one of several treatment groups, which varied along three key dimensions: the match ratio offered (1:1, 2:1, or 3:1), the maximum size of the matching grant ($25,000, $50,000, $100,000, or unstated), and the suggested donation amount (based on prior giving history, scaled by 1.0, 1.25, or 1.5). The dataset records whether the individual made a donation (gave) and how much they donated (amount). It also contains detailed prior giving history, including the number of past donations, recency of giving, and highest previous contribution. Demographic information is included at the zip-code level, such as racial composition (pwhite, pblack), median household income, average household size, homeownership rates, and educational attainment. Additionally, political context is captured through indicators for whether the individual lived in a red or blue state or county during the 2004 presidential election. This rich set of variables enables analysis not only of overall treatment effects but also of heterogeneous effects across political and demographic subgroups.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\n# ========== mrm2 ==========\ncat(\"### Linear Regression: mrm2 ~ treatment\\n\\n\")\n\n### Linear Regression: mrm2 ~ treatment\n\nmodel_mrm2 &lt;- lm(mrm2 ~ treatment, data = df)\nprintCoefmat(summary(model_mrm2)$coefficients, signif.stars = TRUE)\n\n             Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept) 12.998142   0.093526 138.9789   &lt;2e-16 ***\ntreatment    0.013686   0.114534   0.1195   0.9049    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# ========== female ==========\ncat(\"### Linear Regression: female ~ treatment\\n\\n\")\n\n### Linear Regression: female ~ treatment\n\nmodel_female &lt;- lm(female ~ treatment, data = df)\nprintCoefmat(summary(model_female)$coefficients, signif.stars = TRUE)\n\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.2826978  0.0035036 80.6879  &lt; 2e-16 ***\ntreatment   -0.0075469  0.0042920 -1.7584  0.07869 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# ========== years ==========\ncat(\"### Linear Regression: years ~ treatment\\n\\n\")\n\n### Linear Regression: years ~ treatment\n\nmodel_years &lt;- lm(years ~ treatment, data = df)\nprintCoefmat(summary(model_years)$coefficients, signif.stars = TRUE)\n\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.135914   0.042604 144.023   &lt;2e-16 ***\ntreatment   -0.057549   0.052173  -1.103     0.27    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTo test whether the treatment and control groups were balanced before the experiment began, I ran both a t-test and a linear regression using the variable mrm2, which represents months since last donation. The t-test yielded a t-statistic of approximately 0.12, indicating no statistically significant difference between the groups. I then ran a linear regression of mrm2 on the treatment variable. The coefficient on treatment was nearly zero and not statistically significant, confirming the same result: the groups were balanced. This matches the purpose of Table 1 in the paper, which is to verify that the randomization was successful and that the treatment groups were similar before the intervention. This helps isolate the causal effect of the fundraising treatments observed later in the study."
  },
  {
    "objectID": "blog/project1/index.html#experimental-results",
    "href": "blog/project1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n# summary table\nlibrary(ggplot2)\nlibrary(dplyr)\ndonation_rates &lt;- df %&gt;%\n  group_by(treatment) %&gt;%\n  summarise(proportion_donated = mean(gave, na.rm = TRUE)) %&gt;%\n  mutate(group = ifelse(treatment == 1, \"Treatment\", \"Control\"))\n\n# plot\nggplot(donation_rates, aes(x = group, y = proportion_donated)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(\n    title = \"Proportion of People Who Donated\",\n    x = \"\",\n    y = \"Proportion Donated\"\n  ) +\n  scale_y_continuous(\n    breaks = seq(0, 0.03, by = 0.005),\n    limits = c(0, max(donation_rates$proportion_donated) + 0.005)\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5)\n  )\n\n\n\n\nThis bar plot shows the proportion of people who donated in each group. The treatment group, which received a fundraising letter mentioning a matching donation, had a slightly higher donation rate than the control group. While both rates are low overall, the visual difference suggests that even a small change in how the request was framed — in this case, offering a match — may have encouraged more people to give.\n# --- Manual t-test ---\ngave_treat &lt;- df$gave[df$treatment == 1]\ngave_control &lt;- df$gave[df$treatment == 0]\n\nmean_treat &lt;- mean(gave_treat, na.rm = TRUE)\nmean_control &lt;- mean(gave_control, na.rm = TRUE)\n\nvar_treat &lt;- mean_treat * (1 - mean_treat)\nvar_control &lt;- mean_control * (1 - mean_control)\n\nn_treat &lt;- sum(!is.na(gave_treat))\nn_control &lt;- sum(!is.na(gave_control))\n\nt_stat &lt;- (mean_treat - mean_control) / sqrt((var_treat / n_treat) + (var_control / n_control))\np_value &lt;- 2 * pt(-abs(t_stat), df = min(n_treat, n_control) - 1)\nCalculated t-statistic: 3.2095\np-value: 0.0013\nLinear Regression: gave ~ treatment\n             Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 0.0178582  0.0011007 16.2246 &lt; 2.2e-16 ***\ntreatment   0.0041804  0.0013479  3.1014  0.001927 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nI compared the proportion of people who donated in the treatment and control groups. In the control group, about 1.8% of individuals donated, compared to 2.2% in the treatment group. Although this seems like a small difference, a statistical test showed that it is highly unlikely to have occurred by chance. I confirmed this result using both a t-test (following the class formula) and a simple linear regression. Both methods showed the same result — that people who received a matching grant letter were significantly more likely to give.\nThis tells us something important about human behavior: even a small change in how a request is framed — like offering to match someone’s donation — can meaningfully affect whether they decide to act. The offer likely made giving feel more impactful or urgent. This kind of result is exactly what Table 2A in the paper is highlighting: how relatively subtle changes in message framing can lead to statistically and practically meaningful behavior change.\nProbit Regression: gave ~ treatment\n             Estimate Std. Error z value  Pr(&gt;|z|)    \n(Intercept) -2.100141   0.023316 -90.074 &lt; 2.2e-16 ***\ntreatment    0.086785   0.027878   3.113  0.001852 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nI ran a probit regression where the outcome variable was whether an individual donated (gave) and the explanatory variable was whether they were assigned to a treatment group (treatment). The coefficient on treatment was small but positive and statistically significant, replicating the result shown in Table 3, Column 1 of the Karlan & List paper. This suggests that receiving a letter with a matching grant offer had a statistically significant effect on the probability of donating, even after accounting for the binary nature of the outcome variable using a probit model.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n# Filter to match groups\nmatch_data &lt;- df %&gt;% filter(ratio %in% c(\"1\", \"2\", \"3\"))\n\n# 2:1 vs 1:1\nmatch_1v2 &lt;- match_data %&gt;% filter(ratio %in% c(\"1\", \"2\"))\ntt_1v2 &lt;- t.test(gave ~ ratio, data = match_1v2)\n\n# 3:1 vs 1:1\nmatch_1v3 &lt;- match_data %&gt;% filter(ratio %in% c(\"1\", \"3\"))\ntt_1v3 &lt;- t.test(gave ~ ratio, data = match_1v3)\n\n# 3:1 vs 2:1\nmatch_2v3 &lt;- match_data %&gt;% filter(ratio %in% c(\"2\", \"3\"))\ntt_2v3 &lt;- t.test(gave ~ ratio, data = match_2v3)\n2:1 vs 1:1\n  t-statistic:  -0.965 \n  p-value:      0.3345 \n\n3:1 vs 1:1\n  t-statistic:  -1.015 \n  p-value:      0.3101 \n\n3:1 vs 2:1\n  t-statistic:  -0.0501 \n  p-value:      0.96 \nTo test whether larger match ratios increase the likelihood of donating, I ran a series of t-tests comparing donation rates between the 1:1, 2:1, and 3:1 match groups. The results showed no statistically significant differences. The 2:1 and 3:1 groups had slightly higher donation rates than the 1:1 group (about 2.27% vs. 2.07%), but these differences were small and not statistically meaningful (p-values &gt; 0.30). A comparison between the 2:1 and 3:1 groups yielded almost identical results, with a p-value of 0.96. These findings support the authors’ claim on page 8 that increasing the match ratio beyond 1:1 does not further increase giving. This suggests that the presence of a match may be what matters most — once donors see their contribution will be matched, making the match larger does not add much persuasive power.\nLinear Regression: gave ~ ratio\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.0207491  0.0013914 14.9122   &lt;2e-16 ***\nratio2      0.0018843  0.0019677  0.9576   0.3383    \nratio3      0.0019843  0.0019679  1.0083   0.3133    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nIn the regression model comparing match ratios, the 1:1 match group had a baseline donation rate of approximately 2.07%. The 2:1 and 3:1 match groups showed slightly higher estimated donation rates—about 0.19 and 0.20 percentage points more, respectively—but these differences were not statistically significant. Both coefficients had relatively large standard errors and high p-values (above 0.3), indicating low statistical precision. This means we cannot confidently say that higher match ratios had any real effect beyond the 1:1 match. These results support the conclusion that increasing the match ratio does not significantly influence the likelihood of donating.\n\n# mean donation rates\nmeans_by_ratio &lt;- match_data %&gt;%\n  group_by(ratio) %&gt;%\n  summarise(response_rate = mean(gave, na.rm = TRUE))\nmeans_by_ratio\n\n# A tibble: 3 × 2\n  ratio response_rate\n  &lt;fct&gt;         &lt;dbl&gt;\n1 1            0.0207\n2 2            0.0226\n3 3            0.0227\n\n\n# 2:1 to 1:1\ndiff_2v1 &lt;- means_by_ratio$response_rate[means_by_ratio$ratio == \"2\"] -\n            means_by_ratio$response_rate[means_by_ratio$ratio == \"1\"]\n\n# 3:1 to 2:1\ndiff_3v2 &lt;- means_by_ratio$response_rate[means_by_ratio$ratio == \"3\"] -\n            means_by_ratio$response_rate[means_by_ratio$ratio == \"2\"]\nLinear Regression: gave ~ ratio\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.0207491  0.0013914 14.9122   &lt;2e-16 ***\nratio2      0.0018843  0.0019677  0.9576   0.3383    \nratio3      0.0019843  0.0019679  1.0083   0.3133    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nI calculated the difference in response rates between the 1:1 and 2:1 match groups, and between the 2:1 and 3:1 groups, both directly from the data and using the coefficients from a linear regression. In both cases, the differences were very small—around 0.19 percentage points for 2:1 vs. 1:1, and essentially zero for 3:1 vs. 2:1. These results suggest that increasing the size of the matching donation beyond 1:1 has minimal to no effect on whether people choose to give. The consistency between the raw data and regression confirms that larger match ratios do not meaningfully improve donation rates, reinforcing the idea that simply offering a match may matter more than how generous the match actually is.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n# Filter to donors only (amount is 0 when no donation was made)\ndonors_only &lt;- df %&gt;% filter(gave == 1)\n\n# Manual t-test\nt_test_amount &lt;- t.test(amount ~ treatment, data = donors_only)\nLinear Regression: amount ~ treatment (conditional on gave == 1)\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  45.5403     2.4234 18.7921   &lt;2e-16 ***\ntreatment    -1.6684     2.8724 -0.5808   0.5615    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nCalculated t-statistic: 0.5846\np-value:  0.559 \nThis analysis shows that, although people in the treatment group were more likely to donate (as shown earlier), those who did donate did not give more than donors in the control group. In fact, if anything, they gave slightly less — but this difference is small and not statistically meaningful. This tells us that the treatment — the inclusion of a matching donation offer — appears to influence the decision to donate, but not the amount given once someone has already made that decision. In other words, framing boosts participation, but not generosity.\n# donors only\ndonors_only &lt;- match_data %&gt;% filter(gave == 1)\ndonors_only$ratio &lt;- factor(donors_only$ratio)\nLinear Regression: amount ~ ratio (conditional on gave == 1)\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 45.14286    2.76540 16.3242   &lt;2e-16 ***\nratio2       0.19444    3.82852  0.0508   0.9595    \nratio3      -3.89108    3.82490 -1.0173   0.3093    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nI ran a regression to examine whether the size of the match ratio influenced how much people donated, among those who chose to give. The results showed no statistically significant differences between the 1:1, 2:1, and 3:1 groups. Donors in the 2:1 group gave about 19 cents more than the 1:1 group, and donors in the 3:1 group gave about $3.89 less, but neither difference was meaningful or statistically reliable. However, this regression does not have a valid causal interpretation, since it only includes people who donated — a post-treatment outcome that breaks the randomization. As a result, we cannot conclude whether the match ratio itself caused any differences in donation amounts.\n\n# treatment group histogram\ndonors_treat &lt;- donors_only %&gt;% filter(treatment == 1)\nmean_treat &lt;- mean(donors_treat$amount, na.rm = TRUE)\nggplot(donors_treat, aes(x = amount)) +\n  geom_histogram(binwidth = 5, fill = \"skyblue\", color = \"white\") +\n  geom_vline(xintercept = mean_treat, color = \"red\", linetype = \"dashed\", size = 1) +\n  labs(\n    title = \"Donations Among Treatment Group (Gave == 1)\",\n    x = \"Donation Amount ($)\",\n    y = \"Number of Donors\"\n  ) +\n  annotate(\"text\", x = mean_treat, y = 5,\n           label = paste0(\"Mean = $\", round(mean_treat, 2)),\n           color = \"red\", vjust = -0.5) +\n  xlim(0, max(donors_treat$amount, na.rm = TRUE) + 10) +\n  theme_minimal()\n\n\n\n# regenerate control data\ndonors_only &lt;- df %&gt;% filter(gave == 1)  # from full df, not match_data\ndonors_control &lt;- donors_only %&gt;% filter(treatment == 0)\nmean_control &lt;- mean(donors_control$amount, na.rm = TRUE)\n\n# control group histogram\nggplot(donors_control, aes(x = amount)) +\n  geom_histogram(binwidth = 5, fill = \"lightgreen\", color = \"white\") +\n  geom_vline(xintercept = mean_control, color = \"red\", linetype = \"dashed\", size = 1) +\n  labs(\n    title = \"Donations Among Control Group (Gave == 1)\",\n    x = \"Donation Amount ($)\",\n    y = \"Number of Donors\"\n  ) +\n  annotate(\"text\", x = mean_control, y = 5,\n           label = paste0(\"Mean = $\", round(mean_control, 2)),\n           color = \"red\", vjust = -0.5) +\n  xlim(0, max(donors_control$amount, na.rm = TRUE) + 10) +\n  theme_minimal()\n\n\n\n\nWhile the treatment appears to have increased the number of people who gave (as shown earlier), these plots show that it did not meaningfully shift the amount donated among those who did give. Most donors gave similar amounts regardless of treatment status, reinforcing the idea that the matching offer influenced whether someone gave more than how much they gave."
  },
  {
    "objectID": "blog/project1/index.html#simulation-experiment",
    "href": "blog/project1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\n# Set seed for reproducibility\nset.seed(42)\n\n# Simulate control group (Bernoulli with p = 0.018)\ncontrol_sim &lt;- rbinom(n = 100000, size = 1, prob = 0.018)\n\n# Simulate treatment group (Bernoulli with p = 0.022)\ntreatment_sim &lt;- rbinom(n = 10000, size = 1, prob = 0.022)\n\n# Calculate differences using first 10,000 control samples\ndiff_vector &lt;- treatment_sim - control_sim[1:10000]\n\n# Cumulative average of differences\ncumulative_avg &lt;- cumsum(diff_vector) / seq_along(diff_vector)\n\n# True difference\ntrue_diff &lt;- 0.022 - 0.018\n\n# Plot\nplot(cumulative_avg, type = \"l\", col = \"blue\", lwd = 2,\n     main = \"Law of Large Numbers: Cumulative Avg of Bernoulli Differences\",\n     xlab = \"Number of Simulated Samples\",\n     ylab = \"Cumulative Average Difference\")\nabline(h = true_diff, col = \"red\", lty = 2)\nlegend(\"bottomright\", legend = paste0(\"True Difference = \", round(true_diff, 3)), \n       col = \"red\", lty = 2)\n\n\n\n\nThis plot shows the cumulative average of differences in donation outcomes between the treatment and control groups, based on simulated data. I drew 10,000 values from the treatment group distribution and compared each to a randomly drawn value from the control group distribution. Each point on the blue line represents the average difference after one more simulated pair. As expected, the early differences are highly variable, but the line stabilizes as more data accumulate. The red dashed line represents the true difference in probabilities between the groups. The fact that the blue line converges to this value illustrates a key concept in statistics: with enough data, sampling variation averages out, and the sample-based estimate approaches the true effect.\n\n\nCentral Limit Theorem\n\nlibrary(ggplot2)\n\n# Parameters\nsample_sizes &lt;- c(50, 200, 500, 1000)\nsimulations &lt;- 1000\np_control &lt;- 0.018\np_treatment &lt;- 0.022\nset.seed(42)\n\n# Function to simulate average differences\nsimulate_diff_dist &lt;- function(n, reps = 1000) {\n  replicate(reps, {\n    control_draw &lt;- rbinom(n, 1, p_control)\n    treatment_draw &lt;- rbinom(n, 1, p_treatment)\n    mean(treatment_draw) - mean(control_draw)\n  })\n}\n\n# Plot results\npar(mfrow = c(2, 2))  # 2x2 grid\n\nfor (n in sample_sizes) {\n  diffs &lt;- simulate_diff_dist(n)\n  hist(diffs,\n       breaks = 30,\n       main = paste(\"Sample Size =\", n),\n       xlab = \"Avg Treatment - Control\",\n       col = \"lightblue\",\n       border = \"white\")\n  abline(v = 0, col = \"red\", lwd = 2, lty = 2)\n}\n\n\n\n\nThese histograms illustrate the distribution of simulated treatment effects—the average difference in donation rates between treatment and control groups—across varying sample sizes: 50, 200, 500, and 1000. In each plot, the red dashed line marks zero, representing the null hypothesis of no difference. At smaller sample sizes (n = 50 and 200), zero appears near the center of the distribution, indicating high variability and the possibility of observing no effect purely by chance. As the sample size increases (n = 500 and 1000), the distribution narrows, and zero shifts toward the tails. This suggests that with more data, estimates become more precise and it becomes less likely to observe a sample difference near zero if a true effect exists. These results demonstrate the Central Limit Theorem and highlight how larger samples improve our ability to detect real treatment effects and reject the null hypothesis when appropriate."
  },
  {
    "objectID": "blog/project2/index.html#blueprinty-case-study",
    "href": "blog/project2/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\nCode\n# histogram of patents\nggplot(blueprinty, aes(x = patents, fill = factor(iscustomer))) +\n  geom_histogram(binwidth = 1, position = \"dodge\", alpha = 0.7) +\n  labs(\n    title = \"Histogram of Patents by Customer Status\",\n    x = \"Number of Patents\",\n    y = \"Count\",\n    fill = \"Customer Status\"\n  ) +\n  scale_fill_manual(values = c(\"0\" = \"gray\", \"1\" = \"steelblue\"),\n                    labels = c(\"0\" = \"Non-Customer\", \"1\" = \"Customer\"))\n\n\n\n\n\n\n\nCode\n# average patents table\nblueprinty %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarize(mean_patents = mean(patents, na.rm = TRUE)) %&gt;%\n  mutate(\n    iscustomer = ifelse(iscustomer == 1, \"Customer\", \"Non-Customer\")\n  ) %&gt;%\n  kable(digits = 2, col.names = c(\"Customer Status\", \"Average Number of Patents\"),\n        caption = \"Average Patents by Customer Status\")\n\n\n\nAverage Patents by Customer Status\n\n\nCustomer Status\nAverage Number of Patents\n\n\n\n\nNon-Customer\n3.47\n\n\nCustomer\n4.13\n\n\n\n\n\nCustomers tend to have a higher number of patents than non-customers. The average for customers is 4.13 patents, compared to 3.47 for non-customers. The histogram shows that customers are more likely to have higher patent counts, with their distribution shifted to the right. Non-customers are more concentrated in the 0–3 patent range, while customers are more represented in the 4–8 range. This suggests a positive relationship between patent ownership and the likelihood of being a customer.\n\n\nCode\n# boxplot age distribution\nggplot(blueprinty, aes(x = factor(iscustomer), y = age, fill = factor(iscustomer))) +\n  geom_boxplot() +\n  labs(\n    title = \"Age Distribution by Customer Status\",\n    x = \"Customer Status\",\n    y = \"Age\",\n    fill = \"Customer\"\n  ) +\n  scale_fill_manual(values = c(\"0\" = \"gray\", \"1\" = \"steelblue\"),\n                    labels = c(\"0\" = \"Non-Customer\", \"1\" = \"Customer\"))\n\n\n\n\n\n\n\nCode\n# mean age table\nblueprinty %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarize(mean_age = mean(age, na.rm = TRUE)) %&gt;%\n  mutate(\n    iscustomer = ifelse(iscustomer == 1, \"Customer\", \"Non-Customer\")\n  ) %&gt;%\n  kable(digits = 2, col.names = c(\"Customer Status\", \"Average Age\"),\n        caption = \"Average Age by Customer Status\")\n\n\n\nAverage Age by Customer Status\n\n\nCustomer Status\nAverage Age\n\n\n\n\nNon-Customer\n26.1\n\n\nCustomer\n26.9\n\n\n\n\n\nCustomers are slightly older than non-customers on average (26.9 vs. 26.1 years). The boxplot shows similar distributions, but customers have a slightly higher median and a longer upper tail. This suggests that customer status is somewhat associated with age, though the difference is modest.\n\n\nCode\n# proportions for each region by customer status\nregion_customer_prop &lt;- blueprinty %&gt;%\n  group_by(region, iscustomer) %&gt;%\n  summarise(count = n(), .groups = \"drop\") %&gt;%\n  group_by(region) %&gt;%\n  mutate(prop = count / sum(count)) %&gt;%\n  ungroup() %&gt;%\n  mutate(iscustomer = factor(iscustomer, labels = c(\"Non-Customer\", \"Customer\")))\n\n# bar plot \nggplot(region_customer_prop, aes(x = region, y = prop, fill = iscustomer)) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.7)) +\n  labs(\n    title = \"Proportion of Customers and Non-Customers by Region\",\n    x = \"Region\",\n    y = \"Proportion\",\n    fill = \"Customer Status\"\n  ) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme_minimal()\n\n\n\n\n\nRegional differences are more pronounced. In the Northeast, over half of the firms are customers — a much higher proportion than any other region. In contrast, regions like the Midwest, Northwest, South, and Southwest have much lower customer proportions (generally below 20%). This suggests strong regional variation in customer conversion, with the Northeast standing out as Blueprinty’s most successful market.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nAssume we have ( n ) independent observations ( Y_1, Y_2, , Y_n ), each distributed as:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda)\n\\]\nThe probability mass function is:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nSince the observations are independent, the likelihood function is the product of their individual densities:\n\\[\n\\mathcal{L}(\\lambda \\mid Y_1, \\dots, Y_n) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nThis simplifies to:\n\\[\n\\mathcal{L}(\\lambda) = \\frac{e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n Y_i}}{\\prod_{i=1}^n Y_i!}\n\\]\n\n\nCode\n# log-likelihood function for Poisson model\npoisson_loglikelihood &lt;- function(lambda, Y) {\n  if (lambda &lt;= 0) return(-Inf)  # Invalid for log\n  n &lt;- length(Y)\n  ll &lt;- -n * lambda + sum(Y) * log(lambda) - sum(lgamma(Y + 1))\n  return(ll)\n}\n\n\n\n\nCode\n# use the observed number \ny_data &lt;- blueprinty$patents\n\n# define a sequence of lambda values\nlambda_vals &lt;- seq(0.1, 10, length.out = 200)\n\n# log-likelihoods\nlog_likelihoods &lt;- sapply(lambda_vals, function(l) poisson_loglikelihood(l, y_data))\n\n# plot\nplot(lambda_vals, log_likelihoods, type = \"l\",\n     main = \"Log-Likelihood of Poisson Model\",\n     xlab = expression(lambda),\n     ylab = \"Log-Likelihood\",\n     col = \"steelblue\", lwd = 2)\n\n\n\n\n\nWe start with the log-likelihood function for ( n ) independent observations from a Poisson distribution:\n\\[\n\\log \\mathcal{L}(\\lambda) = -n\\lambda + \\left(\\sum_{i=1}^n Y_i\\right) \\log \\lambda - \\sum_{i=1}^n \\log(Y_i!)\n\\]\nTaking the derivative with respect to ( ):\n\\[\n\\frac{d}{d\\lambda} \\log \\mathcal{L}(\\lambda) = -n + \\frac{\\sum_{i=1}^n Y_i}{\\lambda}\n\\]\nSetting the derivative equal to zero:\n\\[\n-n + \\frac{\\sum_{i=1}^n Y_i}{\\lambda} = 0\n\\quad \\Rightarrow \\quad\n\\hat{\\lambda} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n\\]\nThis result makes intuitive sense, as the mean of a Poisson distribution is (), so the MLE is simply the sample mean.\n\n\nCode\n# negative log-likelihood wrapper for use with optim()\nneg_log_likelihood_poisson &lt;- function(lambda, Y) {\n  if (lambda &lt;= 0) return(Inf)\n  -poisson_loglikelihood(lambda, Y)\n}\n\n# use optim to find MLE\noptim_result &lt;- optim(\n  par = 1,  # initial guess\n  fn = neg_log_likelihood_poisson,\n  Y = blueprinty$patents,\n  method = \"Brent\",\n  lower = 0.001,\n  upper = 10\n)\n\n# extract MLE estimate\nlambda_mle_optim &lt;- optim_result$par\ncat(\"MLE estimate of lambda:\", round(lambda_mle_optim, 3), \"\\n\")\n\n\nMLE estimate of lambda: 3.685 \n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\nCode\n# log-likelihood for Poisson regression\npoisson_regression_loglik &lt;- function(beta, Y, X) {\n  eta &lt;- X %*% beta                    \n  lambda &lt;- exp(eta)                  \n  ll &lt;- sum(Y * log(lambda) - lambda - lgamma(Y + 1))\n  return(ll)\n}\n\n\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\n\n\nCode\nblueprinty$age2 &lt;- blueprinty$age^2\n\n# region dummy variables, dropping one region\nregion_dummies &lt;- model.matrix(~ region, data = blueprinty)[, -1]  # drops intercept & first level\n\n# matrix\nX &lt;- cbind(Intercept = 1,\n           age = blueprinty$age,\n           age2 = blueprinty$age2,\n           region_dummies,\n           iscustomer = blueprinty$iscustomer)\n\nY &lt;- blueprinty$patents\n\n# define log-likelihood function\nneg_loglik &lt;- function(beta, Y, X) {\n  eta &lt;- X %*% beta\n  lambda &lt;- exp(eta)\n  -sum(Y * log(lambda) - lambda - lgamma(Y + 1))  # negative\n}\n\n# optimize using optim()\ninit_beta &lt;- rep(0, ncol(X))  \nfit &lt;- optim(\n  par = init_beta,\n  fn = neg_loglik,\n  Y = Y,\n  X = X,\n  hessian = TRUE,\n  method = \"BFGS\"\n)\n\n# extract coefficients & standard errors\nbeta_hat &lt;- fit$par\nhessian &lt;- fit$hessian\nse_beta &lt;- sqrt(diag(solve(hessian)))\n\n# fix possible duplicate column names\ncolnames_X &lt;- make.names(colnames(X), unique = TRUE)\n\n# create & display coefficient table\ncoef_table &lt;- data.frame(\n  Term = colnames_X,\n  Coefficient = beta_hat,\n  Std_Error = se_beta\n)\n\nknitr::kable(coef_table, digits = 3, caption = \"Poisson Regression Coefficients and Standard Errors\")\n\n\n\nPoisson Regression Coefficients and Standard Errors\n\n\nTerm\nCoefficient\nStd_Error\n\n\n\n\nIntercept\n-0.126\n0.112\n\n\nage\n0.116\n0.006\n\n\nage2\n-0.002\n0.000\n\n\nregionNortheast\n-0.025\n0.043\n\n\nregionNorthwest\n-0.035\n0.053\n\n\nregionSouth\n-0.005\n0.052\n\n\nregionSouthwest\n-0.038\n0.047\n\n\niscustomer\n0.061\n0.032\n\n\n\n\n\n\n\nCode\n# fit Poisson regression using glm()\nglm_fit &lt;- glm(\n  patents ~ age + I(age^2) + region + iscustomer,\n  data = blueprinty,\n  family = poisson(link = \"log\")\n)\n\nsummary(glm_fit)\n\n\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + iscustomer, \n    family = poisson(link = \"log\"), data = blueprinty)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.508920   0.183179  -2.778  0.00546 ** \nage              0.148619   0.013869  10.716  &lt; 2e-16 ***\nI(age^2)        -0.002971   0.000258 -11.513  &lt; 2e-16 ***\nregionNortheast  0.029170   0.043625   0.669  0.50372    \nregionNorthwest -0.017574   0.053781  -0.327  0.74383    \nregionSouth      0.056561   0.052662   1.074  0.28281    \nregionSouthwest  0.050576   0.047198   1.072  0.28391    \niscustomer       0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n\n\nThe Poisson regression results show that age and customer status are strong predictors of patent counts. Age has a positive effect, but the negative coefficient on age² indicates diminishing returns, suggesting patent activity peaks at a certain age. Being a customer increases expected patent counts by approximately 23%, holding other factors constant. Regional effects are not statistically significant, indicating that after adjusting for age and customer status, geography does not explain substantial variation in patent activity.\n\n\n\n\n\n\nPoisson Regression Summary\n\n\n\n\nAge: Significant positive effect with diminishing returns (non-linear curve due to age²)\nCustomer status: Customers have ~23% more patents than non-customers (exp(0.208))\nRegion: No region has a statistically significant effect on patent counts\nModel insight: Patent activity is best explained by age (with a peak) and customer conversion\n\n\n\n\n\nCode\nX_base &lt;- cbind(\n  Intercept = 1,\n  age = blueprinty$age,\n  age2 = blueprinty$age^2,\n  model.matrix(~ region, data = blueprinty)[, -1],  # region dummies (drop intercept)\n  iscustomer = blueprinty$iscustomer\n)\n\n# create datasets\nX_0 &lt;- X_base\nX_0[, \"iscustomer\"] &lt;- 0\n\nX_1 &lt;- X_base\nX_1[, \"iscustomer\"] &lt;- 1\n\n# use beta_hat (from optim) \neta_0 &lt;- X_0 %*% beta_hat\neta_1 &lt;- X_1 %*% beta_hat\n\ny_pred_0 &lt;- exp(eta_0)\ny_pred_1 &lt;- exp(eta_1)\n\n# average difference \navg_diff &lt;- mean(y_pred_1 - y_pred_0)\ncat(\"Estimated average increase:\", round(avg_diff, 3), \"\\n\")\n\n\nEstimated average increase: 0.218"
  },
  {
    "objectID": "blog/project2/index.html#airbnb-case-study",
    "href": "blog/project2/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n\nCode\n# glm() to fit Poisson model\nairbnb_model &lt;- glm(\n  number_of_reviews ~ bathrooms + bedrooms + price +\n    review_scores_cleanliness + review_scores_location + review_scores_value +\n    instant_bookable + room_type,\n  data = airbnb_clean,\n  family = poisson(link = \"log\")\n)\n\nsummary(airbnb_model)\n\n\n\nCall:\nglm(formula = number_of_reviews ~ bathrooms + bedrooms + price + \n    review_scores_cleanliness + review_scores_location + review_scores_value + \n    instant_bookable + room_type, family = poisson(link = \"log\"), \n    data = airbnb_clean)\n\nCoefficients: (1 not defined because of singularities)\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                3.714e+00  1.587e-02 233.976  &lt; 2e-16 ***\nbathrooms                 -1.164e-01  3.786e-03 -30.751  &lt; 2e-16 ***\nbedrooms                   7.607e-02  2.001e-03  38.016  &lt; 2e-16 ***\nprice                     -3.275e-05  8.521e-06  -3.843 0.000121 ***\nreview_scores_cleanliness  1.140e-01  1.486e-03  76.750  &lt; 2e-16 ***\nreview_scores_location    -8.065e-02  1.599e-03 -50.445  &lt; 2e-16 ***\nreview_scores_value       -9.749e-02  1.789e-03 -54.484  &lt; 2e-16 ***\ninstant_bookable                  NA         NA      NA       NA    \nroom_typePrivate room      7.405e-03  2.734e-03   2.709 0.006747 ** \nroom_typeShared room      -2.262e-01  8.616e-03 -26.249  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 961626  on 30159  degrees of freedom\nResidual deviance: 949198  on 30151  degrees of freedom\nAIC: 1070683\n\nNumber of Fisher Scoring iterations: 6\n\n\nThis Poisson regression model examines how listing characteristics affect the number of reviews, used here as a proxy for bookings. The results show that bedroom count and review cleanliness scores have strong positive effects on review count, indicating that larger and cleaner listings attract more bookings.\nHigher prices are associated with fewer reviews, suggesting price sensitivity among renters. Interestingly, while location and value scores are typically seen as positive quality signals, they have significant negative coefficients — which may reflect reverse causality, data quality issues, or confounding factors (e.g., higher-scoring locations already having high competition or price).\nRoom type also plays a role: private rooms receive more reviews than entire homes (the baseline), while shared rooms receive fewer. The instant_bookable variable was excluded due to collinearity or lack of variation.\nOverall, the model confirms that listing attributes, epecially quality signals and room configuration, meaningfully influence booking behavior on the platform.\n\n\n\n\n\n\nKey Findings\n\n\n\n\nBathrooms: More bathrooms → fewer reviews (surprising, possibly due to correlation with higher-end/less frequently booked properties)\nBedrooms: More bedrooms → more reviews (larger listings are more popular)\nPrice: Higher price → fewer reviews (suggests price sensitivity among renters)\nReview scores:\n\nCleanliness: Higher score → more reviews (clear signal of quality)\nLocation & Value: Higher scores → fewer reviews (unexpected; may reflect underlying data quirks)\n\nRoom type:\n\nPrivate room → more reviews than entire homes\nShared room → fewer reviews than entire homes\n\nInstant bookable: Dropped due to collinearity or lack of variation; revisit in preprocessing if needed"
  },
  {
    "objectID": "blog/project3 /index.html",
    "href": "blog/project3 /index.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "blog/project3 /index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "blog/project3 /index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "blog/project3 /index.html#simulate-conjoint-data",
    "href": "blog/project3 /index.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data based on the utility specification described above. Each respondent faces a series of choice tasks, and their selections are determined by both the systematic utility from product attributes and a random error term drawn from a Gumbel distribution. This setup mimics how real respondents might make decisions in a discrete choice experiment.\n\n\n\n\n\n\n\n\nCode\n# set seed for reproducibility\nset.seed(123)\n\n# define attributes\nbrand &lt;- c(\"N\", \"P\", \"H\") # Netflix, Prime, Hulu\nad &lt;- c(\"Yes\", \"No\")\nprice &lt;- seq(8, 32, by=4)\n\n# generate all possible profiles\nprofiles &lt;- expand.grid(\n    brand = brand,\n    ad = ad,\n    price = price\n)\nm &lt;- nrow(profiles)\n\n# assign part-worth utilities (true parameters)\nb_util &lt;- c(N = 1.0, P = 0.5, H = 0)\na_util &lt;- c(Yes = -0.8, No = 0.0)\np_util &lt;- function(p) -0.1 * p\n\n# number of respondents, choice tasks, and alternatives per task\nn_peeps &lt;- 100\nn_tasks &lt;- 10\nn_alts &lt;- 3\n\n# function to simulate one respondent’s data\nsim_one &lt;- function(id) {\n  \n    datlist &lt;- list()\n    \n    # loop over choice tasks\n    for (t in 1:n_tasks) {\n        \n        # randomly sample 3 alts (better practice would be to use a design)\n        dat &lt;- cbind(resp=id, task=t, profiles[sample(m, size=n_alts), ])\n        \n        # compute deterministic portion of utility\n        dat$v &lt;- b_util[dat$brand] + a_util[dat$ad] + p_util(dat$price) |&gt; round(10)\n        \n        # add Gumbel noise (Type I extreme value)\n        dat$e &lt;- -log(-log(runif(n_alts)))\n        dat$u &lt;- dat$v + dat$e\n        \n        # identify chosen alternative\n        dat$choice &lt;- as.integer(dat$u == max(dat$u))\n        \n        # store task\n        datlist[[t]] &lt;- dat\n    }\n    \n    # combine all tasks for one respondent\n    do.call(rbind, datlist)\n}\n\n# simulate data for all respondents\nconjoint_data &lt;- do.call(rbind, lapply(1:n_peeps, sim_one))\n\n# remove values unobservable to the researcher\nconjoint_data &lt;- conjoint_data[ , c(\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\")]\n\n# clean up\nrm(list=setdiff(ls(), \"conjoint_data\"))"
  },
  {
    "objectID": "blog/project3 /index.html#preparing-the-data-for-estimation",
    "href": "blog/project3 /index.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\nIn order to estimate the MNL model, we need to reshape the data so that it includes one row per alternative, per choice task, per respondent. We also convert categorical variables (brand and ad format) into binary indicators using dummy coding, with Hulu and ad-free as the reference categories. This format allows us to calculate utility values and model choice behavior based on attribute-level part-worths.\n\n\nCode\n# required libraries\nlibrary(dplyr)\nlibrary(tidyr)\n\n# work with simulated conjoint_data\ndf &lt;- conjoint_data\n\n# convert to factors with reference levels\ndf$brand &lt;- relevel(factor(df$brand), ref = \"H\")  # Hulu is reference\ndf$ad &lt;- relevel(factor(df$ad), ref = \"No\")       # No ads is reference\n\n# create dummy variables, no intercept to avoid collinearity\nX &lt;- model.matrix(~ brand + ad - 1, data = df)\nX &lt;- as.data.frame(X)\n\n# rename to match previous convention\ncolnames(X) &lt;- gsub(\"brandN\", \"brand_N\", colnames(X))\ncolnames(X) &lt;- gsub(\"brandP\", \"brand_P\", colnames(X))\ncolnames(X) &lt;- gsub(\"adYes\", \"ad_Yes\", colnames(X))  # after setting \"No\" as reference, this should now appear\n\n# combine and reorder\ndf &lt;- bind_cols(df %&gt;% select(resp, task, price, choice), X)\ndf &lt;- df %&gt;% select(resp, task, brand_N, brand_P, ad_Yes, price, choice)"
  },
  {
    "objectID": "blog/project3 /index.html#estimation-via-maximum-likelihood",
    "href": "blog/project3 /index.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\nTo estimate the parameters of the MNL model, we define a log-likelihood function that calculates the probability of each observed choice based on a given set of β β coefficients. The function computes the utility of each alternative, transforms it into a probability using the softmax formula, and sums the log-probabilities of the chosen alternatives. This function is then used in optimization to find the best-fitting parameters.\n\n\nCode\n# log-likelihood function for MNL\nlog_likelihood &lt;- function(beta, data) {\n  # unpack beta coefficients\n  b_brand_N &lt;- beta[1]\n  b_brand_P &lt;- beta[2]\n  b_ad_Yes  &lt;- beta[3]\n  b_price   &lt;- beta[4]\n  \n  # compute linear utility for all alternatives\n  data$v &lt;- b_brand_N * data$brand_N +\n            b_brand_P * data$brand_P +\n            b_ad_Yes  * data$ad_Yes +\n            b_price   * data$price\n  \n  # normalize by task (3 alternatives per task)\n  data &lt;- data %&gt;% \n    group_by(resp, task) %&gt;%\n    mutate(\n      exp_v = exp(v),\n      denom = sum(exp_v),\n      p = exp_v / denom,\n      logp = log(p)\n    ) %&gt;%\n    ungroup()\n  \n  # compute total log-likelihood over chosen alternatives\n  ll &lt;- sum(data$choice * data$logp)\n  return(-ll)  # return negative log-likelihood for minimization\n}\n\n\nWith the log-likelihood function defined, we now use the optim() function in R to find the maximum likelihood estimates of the four model parameters. The optimization uses the BFGS method and returns both the coefficient estimates and the Hessian matrix. We use the inverse of the Hessian to calculate standard errors and construct 95% confidence intervals for each parameter.\n\n\nCode\n# initial guess for the parameters\nstart_vals &lt;- c(0, 0, 0, 0)  # brand_N, brand_P, ad_Yes, price\n\n# run optimization using BFGS (returns Hessian)\nmle_result &lt;- optim(par = start_vals,\n                    fn = log_likelihood,\n                    data = df,\n                    method = \"BFGS\",\n                    hessian = TRUE)\n\n# extract estimated coefficients\nbeta_hat &lt;- mle_result$par\n\n# compute standard errors from inverse Hessian\nvar_cov_matrix &lt;- solve(mle_result$hessian)\nstd_errors &lt;- sqrt(diag(var_cov_matrix))\n\n# 95% confidence intervals\nz_value &lt;- qnorm(0.975)  # for 95% CI\nlower_ci &lt;- beta_hat - z_value * std_errors\nupper_ci &lt;- beta_hat + z_value * std_errors\n\n# build  summary table\nmle_summary &lt;- data.frame(\n  Parameter = c(\"brand_N\", \"brand_P\", \"ad_Yes\", \"price\"),\n  Estimate = beta_hat,\n  StdError = std_errors,\n  CI_Lower = lower_ci,\n  CI_Upper = upper_ci\n)\n\nmle_summary &lt;- mle_summary %&gt;%\n  mutate(across(Estimate:CI_Upper))\n\nknitr::kable(mle_summary, caption = \"Maximum Likelihood Estimates with 95% Confidence Intervals\")\n\n\n\nMaximum Likelihood Estimates with 95% Confidence Intervals\n\n\nParameter\nEstimate\nStdError\nCI_Lower\nCI_Upper\n\n\n\n\nbrand_N\n0.9412047\n0.1110396\n0.7235710\n1.1588384\n\n\nbrand_P\n0.5016170\n0.1111000\n0.2838650\n0.7193690\n\n\nad_Yes\n-0.7320014\n0.0878097\n-0.9041053\n-0.5598976\n\n\nprice\n-0.0994816\n0.0063336\n-0.1118953\n-0.0870678"
  },
  {
    "objectID": "blog/project3 /index.html#estimation-via-bayesian-methods",
    "href": "blog/project3 /index.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\nWe now estimate the MNL model using a Bayesian approach via the Metropolis-Hastings MCMC algorithm. We define weakly informative priors for the coefficients and generate 11,000 samples from the posterior distribution, discarding the first 1,000 as burn-in. The log-likelihood function from the MLE step is reused, and posterior draws are accepted or rejected based on the Metropolis-Hastings criterion. This approach gives us a full distribution for each parameter rather than just point estimates.\n\n\nCode\n# log prior\nlog_prior &lt;- function(beta) {\n  # brand_N, brand_P, ad_Yes ~ N(0, 5); price ~ N(0, 1)\n  lp &lt;- dnorm(beta[1], 0, sqrt(5), log = TRUE) +\n        dnorm(beta[2], 0, sqrt(5), log = TRUE) +\n        dnorm(beta[3], 0, sqrt(5), log = TRUE) +\n        dnorm(beta[4], 0, 1, log = TRUE)\n  return(lp)\n}\n\n# log posterior = log-likelihood + log-prior\nlog_posterior &lt;- function(beta, data) {\n  -log_likelihood(beta, data) + log_prior(beta)\n}\n\n# mcmc settings\nn_iter &lt;- 11000\nburn_in &lt;- 1000\nkeep &lt;- n_iter - burn_in\n\n# storage\nbeta_samples &lt;- matrix(NA, nrow = n_iter, ncol = 4)\ncolnames(beta_samples) &lt;- c(\"brand_N\", \"brand_P\", \"ad_Yes\", \"price\")\n\n# initialize\nbeta_current &lt;- c(0, 0, 0, 0)\nlog_post_current &lt;- log_posterior(beta_current, df)\n\n# proposal sd (step size)\nproposal_sd &lt;- c(0.05, 0.05, 0.05, 0.005)\n\n# mcmc loop\nfor (i in 1:n_iter) {\n  beta_proposal &lt;- beta_current + rnorm(4, 0, proposal_sd)\n  log_post_proposal &lt;- log_posterior(beta_proposal, df)\n\n  log_accept_ratio &lt;- log_post_proposal - log_post_current\n  accept &lt;- log(runif(1)) &lt; log_accept_ratio\n\n  if (accept) {\n    beta_current &lt;- beta_proposal\n    log_post_current &lt;- log_post_proposal\n  }\n\n  beta_samples[i, ] &lt;- beta_current\n}\n\n# discard burn-in\nposterior_draws &lt;- beta_samples[(burn_in + 1):n_iter, ]\n\n# summarize\nposterior_summary &lt;- apply(posterior_draws, 2, function(x) {\n  c(Mean = mean(x),\n    StdError = sd(x),\n    CI_Lower = quantile(x, 0.025),\n    CI_Upper = quantile(x, 0.975))\n})\nposterior_summary &lt;- as.data.frame(t(posterior_summary))\n\nknitr::kable(posterior_summary,\n             caption = \"Posterior Means, Standard Deviations, and 95% Credible Intervals\")\n\n\n\nPosterior Means, Standard Deviations, and 95% Credible Intervals\n\n\n\nMean\nStdError\nCI_Lower.2.5%\nCI_Upper.97.5%\n\n\n\n\nbrand_N\n0.9412443\n0.1179382\n0.7128011\n1.1688430\n\n\nbrand_P\n0.5015382\n0.1164215\n0.2701299\n0.7223185\n\n\nad_Yes\n-0.7286072\n0.0908280\n-0.9124879\n-0.5540624\n\n\nprice\n-0.0998762\n0.0063300\n-0.1123195\n-0.0873285\n\n\n\n\n\nTo visualize how the MCMC sampler behaved, we plot the trace and histogram for the posterior draws of β price β price ​\n. The trace plot shows how the sampler explored the parameter space over time, while the histogram illustrates the shape and spread of the posterior distribution.\n\n\nCode\n# extract posterior samples for beta_price\nbeta_price_draws &lt;- posterior_draws[, \"price\"]\n\n# set up plotting area: 1 row, 2 plots\npar(mfrow = c(1, 2))\n\n# trace plot\nplot(beta_price_draws, type = \"l\", col = \"blue\",\n     main = \"Trace Plot of β_price\",\n     xlab = \"Iteration\", ylab = \"Value\")\n\n# histogram\nhist(beta_price_draws, breaks = 40, col = \"lightblue\", border = \"white\",\n     main = \"Posterior Distribution of β_price\",\n     xlab = \"β_price\")\n\n\n\n\n\nThe table below compares the parameter estimates obtained using Maximum Likelihood and Bayesian (posterior) methods. For each of the four model parameters, we report the mean estimate, standard deviation (or standard error), and the corresponding 95% confidence or credible intervals. The similarity between the two sets of results indicates that the data were strong enough that the prior had little influence on the posterior.\n\n\nCode\n# posterior summary\nposterior_summary &lt;- apply(posterior_draws, 2, function(x) {\n  c(Mean = mean(x),\n    StdError = sd(x),\n    CI_Lower = quantile(x, 0.025),\n    CI_Upper = quantile(x, 0.975))\n})\nposterior_summary &lt;- as.data.frame(t(posterior_summary))\n\n# mle summary\nstart_vals &lt;- c(0, 0, 0, 0)\nmle_result &lt;- optim(start_vals, log_likelihood, data = df, method = \"BFGS\", hessian = TRUE)\nbeta_hat &lt;- mle_result$par\nvar_cov_matrix &lt;- solve(mle_result$hessian)\nstd_errors &lt;- sqrt(diag(var_cov_matrix))\nz &lt;- qnorm(0.975)\nci_lower &lt;- beta_hat - z * std_errors\nci_upper &lt;- beta_hat + z * std_errors\n\nmle_summary &lt;- data.frame(\n  mean = beta_hat,\n  sd = std_errors,\n  ci_lower = ci_lower,\n  ci_upper = ci_upper\n)\nrownames(mle_summary) &lt;- c(\"brand_N\", \"brand_P\", \"ad_Yes\", \"price\")\n\n# combine: round only MLE values\ncomparison &lt;- data.frame(\n  Parameter = rownames(mle_summary),\n  MLE_Mean = round(mle_summary$mean, 3),\n  MLE_SD = round(mle_summary$sd, 3),\n  MLE_Lower = round(mle_summary$ci_lower, 3),\n  MLE_Upper = round(mle_summary$ci_upper, 3),\n  Post_Mean = posterior_summary$Mean,\n  Post_SD = posterior_summary$StdError,\n  Post_Lower = posterior_summary$CI_Lower,\n  Post_Upper = posterior_summary$CI_Upper\n)\n\n# display table\nknitr::kable(comparison, caption = \"Comparison of MLE and Bayesian Estimates with 95% Intervals\")\n\n\n\nComparison of MLE and Bayesian Estimates with 95% Intervals\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nMLE_Mean\nMLE_SD\nMLE_Lower\nMLE_Upper\nPost_Mean\nPost_SD\nPost_Lower\nPost_Upper\n\n\n\n\nbrand_N\n0.941\n0.111\n0.724\n1.159\n0.9412443\n0.1179382\n0.7128011\n1.1688430\n\n\nbrand_P\n0.502\n0.111\n0.284\n0.719\n0.5015382\n0.1164215\n0.2701299\n0.7223185\n\n\nad_Yes\n-0.732\n0.088\n-0.904\n-0.560\n-0.7286072\n0.0908280\n-0.9124879\n-0.5540624\n\n\nprice\n-0.099\n0.006\n-0.112\n-0.087\n-0.0998762\n0.0063300\n-0.1123195\n-0.0873285\n\n\n\n\n\nThe results from the Bayesian estimation were very similar to those from the maximum likelihood approach. For all four parameters — brand_N, brand_P, ad_Yes, and price — the posterior means from the MCMC sampling were nearly identical to the MLE point estimates. The 95% credible intervals from the Bayesian method also closely matched the 95% confidence intervals from the MLE. For example, the estimated effect of price was –0.0995 under MLE and –0.100 under the Bayesian approach, with intervals that overlapped almost exactly. This consistency suggests that the data were strong and informative enough that the weakly informative priors had little effect on the posterior. The key advantage of the Bayesian method is that it provides full distributions for each parameter, which is especially useful for visualizing uncertainty and interpreting model results more flexibly."
  },
  {
    "objectID": "blog/project3 /index.html#discussion",
    "href": "blog/project3 /index.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\n\nInterpreting the Parameter Estimates\nEven if the data hadn’t been simulated, the parameter estimates make intuitive sense. The fact that \\(\\beta_\\text{Netflix} &gt; \\beta_\\text{Prime}\\) suggests that respondents preferred Netflix over Prime, all else equal. Since Hulu is the reference brand, both of these estimates are interpreted relative to it — meaning Netflix is the most preferred, Prime is somewhat preferred, and Hulu is the least preferred.\nThe negative sign on \\(\\beta_\\text{price}\\) also aligns with expectations: as the monthly price increases, the likelihood of choosing that option decreases. This is consistent with typical consumer behavior — people tend to prefer cheaper options when everything else is held constant.\n\n\nMoving to a Hierarchical (Multi-Level) Model\nTo simulate data from a multi-level or hierarchical model (also called a random-parameter MNL), the main change would be allowing the \\(\\beta\\) coefficients to vary across individuals, rather than assuming they are fixed for everyone. Instead of using a single set of part-worths that apply to all respondents, we assume that each person has their own vector of coefficients, \\(\\beta_i\\), drawn from a population-level distribution — typically something like \\(\\beta_i \\sim \\mathcal{N}(\\mu, \\Sigma)\\), where \\(\\mu\\) is the average preference across respondents and \\(\\Sigma\\) captures the variance and covariances in preferences.\nTo simulate this kind of data, we would draw a unique \\(\\beta_i\\) for each respondent from the population distribution and then use that individual’s \\(\\beta_i\\) to generate their choices across tasks. Estimating this kind of model requires Bayesian methods (like hierarchical Bayes) or simulation-based techniques (like simulated maximum likelihood), since the model no longer has a closed-form likelihood. These models are more realistic for real-world conjoint data because they allow for heterogeneity in preferences — recognizing that not everyone values brand, ads, or price in the same way."
  }
]